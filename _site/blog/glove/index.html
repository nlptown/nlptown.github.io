<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Notable NLP: GloVe &mdash; Global Vectors for Word Representations &#8211; Yves Peirsman</title>
<meta name="description" content="">
<meta name="keywords" content="NLP, semantics, meaning, words, glove, vectors, notable">
<meta name="author" content="Yves Peirsman">
<!-- Twitter Cards -->
	
		<meta name="twitter:card" content="summary">
		<meta name="twitter:image" content=
			
				
						"http://nlp.yvespeirsman.be/images/"
				
			
		>
	
	<meta name="twitter:title" content="Notable NLP: GloVe &mdash; Global Vectors for Word Representations">
	<meta name="twitter:description" content="This is the website and blog of Yves Peirsman, Natural Language Processing Consultant.">
	<meta name="twitter:creator" content="@yvespeirsman">


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Notable NLP: GloVe &mdash; Global Vectors for Word Representations">
<meta property="og:description" content="This is the website and blog of Yves Peirsman, Natural Language Processing Consultant.">
<meta property="og:url" content="http://nlp.yvespeirsman.be/blog/glove/">
<meta property="og:site_name" content="Yves Peirsman">





<link rel="canonical" href="http://nlp.yvespeirsman.be/blog/glove/">
<link href="http://nlp.yvespeirsman.be/feed.xml" type="application/atom+xml" rel="alternate" title="Yves Peirsman Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='http://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://nlp.yvespeirsman.be/assets/css/main.min.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://nlp.yvespeirsman.be/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://nlp.yvespeirsman.be/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://nlp.yvespeirsman.be/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://nlp.yvespeirsman.be/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://nlp.yvespeirsman.be/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://nlp.yvespeirsman.be/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://nlp.yvespeirsman.be/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://nlp.yvespeirsman.be/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://nlp.yvespeirsman.be/images/apple-touch-icon-144x144-precomposed.png">

<link rel="stylesheet" href="/css/yves.css">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>

<body class="post">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
	</div> <!--/.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://nlp.yvespeirsman.be/" >Home</a></li>
		        
				<li><a href="http://nlp.yvespeirsman.be/blog/" >All Posts</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->




<div id="main" role="main">
  <div class="article-author-side">
    
	<a href="http://nlp.yvespeirsman.be"><img src="http://nlp.yvespeirsman.be/images/bio-photo.jpg" class="bio-photo" alt="Yves Peirsman bio photo"></a>

<h3><a href="http://nlp.yvespeirsman.be">Yves Peirsman</a></h3>
<p>Crazy about Natural Language Processing.</p>
<a href="http://twitter.com/yvespeirsman" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>


<a href="http://linkedin.com/in/yvespeirsman" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>










  </div>
  <article>
    <div class="headline-wrap">
      
        <h1><a href="http://nlp.yvespeirsman.be/blog/glove/" rel="bookmark" title="Notable NLP: GloVe &mdash; Global Vectors for Word Representations">Notable NLP: GloVe &mdash; Global Vectors for Word Representations</a></h1>
      
    </div><!--/ .headline-wrap -->
    <div class="article-wrap">
      <p class="first">In NLP research, models of word meaning are making a comeback. While previously, attention seemed to shift from words to phrases and sentences, word meaning is again at the forefront of research. Maybe people have figured out that, before we can capture phrases and sentences correctly, we need better models of word meaning first. One particularly interesting contribution last year came from Stanford, with Jeffrey Pennington, Richard Socher and Christopher D. Manning presenting <a href="http://nlp.stanford.edu/projects/glove/glove.pdf">GloVe: Global Vectors for Word Meaning</a>.</p>

<p>In recent years, two main types of models of word meaning have emerged: <i>matrix factorization methods</i> such as <a href="http://lsa.colorado.edu/papers/dp1.LSAintro.pdf">Latent Semantic Analysis</a>, which work on the basis of global co-occurrence counts of words in a corpus, and <i>local context window methods</i> such as <a href="http://research.microsoft.com/pubs/189726/rvecs.pdf">Mikolov et al.’s skip-gram model</a>, which are trained on local co-occurrences. Pennington et al. position themselves in the first group, as GloVe, short for Global Vectors, relies on global corpus statistics rather than local context.</p>

<p>One thing I like about Pennington et al.’s approach is their commitment to finding word vectors with semantically meaningful dimensions. They do this by building a model that is biased towards  dimensions of meaning that potentially distinguish between two words, such as gender. They start by defining a learning function that gives a value for a combination of two target words and a context word. The ideal outcome of such a function, they claim, is the ratio between the co-occurrence probabilities of those target words with that context word. When a context word is highly correlated with one of the target words, this ratio will be either very high or very low; when it does not distinguish between the words, this ratio will lie around 1. That sounds logical enough.</p>

<p class="nomargin">In addition, Pennington et al. continue, this function <i>F</i> should have some other desirable characteristics:</p>
<ul><li>Because vector spaces are inherently linear, <i>F</i> should depend only on the difference between the vectors of the two target words.</li>
<li><i>F</i>  should not mix the dimensions of the word vectors in undesirable ways.</li>
<li>Because the distinction between a target word and a context word is irrelevant, <i>F</i>  should be invariant when these words are swapped.</li>
</ul>

<p>After identifying a suitable candidate, Pennington et al. use it to factorize an initial global co-occurrence matrix into a matrix with fewer dimensions, which are more semantically meaningful. In particular, they cast the factorization of the co-occurrence matrix as a least-squares problem that involves the logarithm of the original co-occurrence values and a weighting function that manages the impact of rare and frequent co-occurrences.</p>

<p>Through a number of experiments, they then shown that the resulting word vectors perform very well across a variety of tasks, including a word analogy task that requires them to answer questions like “Athens is to Greece as Berlin is to ___”. As with all models of this kind, performance depends heavily on a number of parameter settings: the context window for the co-occurrences has an optimal size of 10 words, and the word vectors ideally have around 300 dimensions.</p>

<p>In addition to the paper, the <a href="http://nlp.stanford.edu/projects/glove/">accompanying website</a> shows that the final word matrix captures a number of semantic distinctions very nicely. For example, word pairs like <i>man-woman</i>, <i>brother-sister</i> and <i>king-queen</i> differ along the same dimensions. There’s only a small number of examples, but it’s really fascinating stuff.</p>

<p><img class="centered" src="http://nlp.stanford.edu/projects/glove/images/man_woman.jpg" height="400" width="400" /></p>

<p>Unsurprisingly, GloVe has generated quite some interest. The code is freely available from <a href="http://nlp.stanford.edu/projects/glove/">the Stanford website</a>, just like several sets of pre-trained word vectors. Someone wrote a <a href="http://www.foldl.me/2014/glove-python/">helpful tutorial for coding GloVe in Python</a>, and there’s <a href="https://github.com/maciejkula/glove-python">another toy implementation</a> on Github. While most people are impressed by GloVe's performance, other reactions have been lukewarm, arguing that <a href="https://docs.google.com/document/d/1ydIujJ7ETSZ688RGfU5IMJJsbxAi-kRl8czSwpti15s/edit#heading=h.66rkmh7nd17u">the GloVe paper makes an unfair comparison with other competing methods</a>.</p>

<p>Lately there has been some kind of arms race for the best word vectors, with several types of models competing. While this is surely a good thing, we shouldn’t forget word vectors can only give an abstract approximation of word meaning, no matter how good they are. The meaning of a word is not just some average of all its uses in a corpus: for each occurrence it crucially depends on the local context in which the word is used. Among all attention that word meaning is now getting, modelling the local meaning of a single occurrence of a word is a challenge I would love to see addressed more often.</p>


      <div class="addthis_sharing_toolbox"></div>
      <hr />
      <footer role="contentinfo">
        <div class="article-author-bottom">
          
	<a href="http://nlp.yvespeirsman.be"><img src="http://nlp.yvespeirsman.be/images/bio-photo.jpg" class="bio-photo" alt="Yves Peirsman bio photo"></a>

<h3><a href="http://nlp.yvespeirsman.be">Yves Peirsman</a></h3>
<p>Crazy about Natural Language Processing.</p>
<a href="http://twitter.com/yvespeirsman" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>


<a href="http://linkedin.com/in/yvespeirsman" class="author-social" target="_blank"><i class="fa fa-linkedin-square"></i> LinkedIn</a>










        </div>
        <p class="byline"><strong>Notable NLP: GloVe &mdash; Global Vectors for Word Representations</strong> was published on <time datetime="2015-01-15T04:00:00-05:00">January 15, 2015</time> by <a href="http://nlp.yvespeirsman.be/about" title="About Yves Peirsman">Yves Peirsman</a>.</p>
      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="http://nlp.yvespeirsman.be/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="http://nlp.yvespeirsman.be/blog/generating-genre-fiction-with-deep-learning/" title="Generating Genre Fiction with Deep Learning">Generating Genre Fiction with Deep Learning</a></li>
    
      <li><a href="http://nlp.yvespeirsman.be/blog/text-retrieval-6/" title="Text Retrieval & Search Engines (Coursera): Week&nbsp;4 Part&nbsp;2: Recommender Systems">Text Retrieval & Search Engines (Coursera): Week&nbsp;4 Part&nbsp;2: Recommender Systems</a></li>
    
      <li><a href="http://nlp.yvespeirsman.be/blog/visualizing-word-embeddings-with-tsne/" title="Visualizing Word Embeddings with t-SNE">Visualizing Word Embeddings with t-SNE</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  <footer>
    <span>&copy; 2015 Yves Peirsman. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->



<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://nlp.yvespeirsman.be/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://nlp.yvespeirsman.be/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53268217-1', 'auto');
  ga('send', 'pageview');
</script>



  <!-- Go to www.addthis.com/dashboard to customize your tools -->
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-53fb5daa449762bb"></script>
  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'yvespeirsman'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



	        

</body>
</html>
