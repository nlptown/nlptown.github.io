<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">NLP Town</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://www.nlp.town/feed.xml" />
<link rel="alternate" type="text/html" href="http://www.nlp.town" />
<updated>2018-08-06T05:16:58-04:00</updated>
<id>http://www.nlp.town/</id>
<author>
  <name>Yves Peirsman</name>
  <uri>http://www.nlp.town/</uri>
  <email></email>
</author>


<entry>
  <title type="html"><![CDATA[Comparing Sentence Similarity Methods]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/sentence-similarity/" />
  <id>http://www.nlp.town/blog/sentence-similarity</id>
  <published>2018-05-02T08:00:00-04:00</published>
  <updated>2018-05-02T08:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
Word embeddings have become widespread in Natural Language Processing. They allow us to easily
compute the semantic similarity between two words, or to find the words most similar to a target word.
However, often we&#39;re more interested in the similarity between two sentences or short texts.
In this blog post, we&#39;ll compare the most popular ways of computing sentence similarity and investigate how they perform.
For people interested in the code, there&#39;s a companion
&lt;a href=&quot;https://github.com/nlptown/sentence-similarity/blob/master/Simple%20Sentence%20Similarity.ipynb&quot;&gt;Jupyter Notebook&lt;/a&gt;
with all the details.
&lt;/p&gt;

&lt;p&gt;Many NLP applications need to compute the similarity in meaning between two short texts. Search engines, for example,
 need to model the
relevance of a document to a query, beyond the overlap in words between the two. Similarly, question-and-answer sites
such as &lt;a href=&quot;https://www.kaggle.com/c/quora-question-pairs&quot;&gt;Quora&lt;/a&gt; need to determine whether a question has already been
asked before. This type of text similarity is often computed by first embedding the two short texts and then
calculating the cosine similarity between them. Although word embeddings such as word2vec and GloVe have
become standard approaches for finding the semantic similarity between two words, there is less agreement
on how sentence embeddings should be computed. Below we’ll review some of the most common methods and compare
their performance on two established benchmarks.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;/images/blog/sensim.png&quot; /&gt;
&lt;figcaption&gt;Sentence similarity is typically calculated by first embedding the sentences and then taking
the cosine similarity between them. (&lt;a href=&quot;https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1&quot;&gt;Source&lt;/a&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;We&#39;ll evaluate all methods on two widely used datasets with human similarity judgements:&lt;/p&gt;

&lt;ul class=&quot;nomargin&quot;&gt;
&lt;li&gt;The &lt;a href=&quot;http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark&quot;&gt;STS Benchmark&lt;/a&gt;
brings together the English data from the SemEval sentence similarity tasks between 2012 and 2017.&lt;/li&gt;
&lt;li&gt;The &lt;a href=&quot;http://clic.cimec.unitn.it/composes/sick.html&quot;&gt;SICK data&lt;/a&gt; contains 10,000 English sentence pairs
labelled with their semantic relatedness and entailment relation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The table below contains a few examples from the STS data. As you can see, the semantic relationship
between the two sentences is often quite subtle: the sentences &lt;code&gt;a man is playing a harp&lt;/code&gt; and &lt;code&gt;a man is playing a keyboard&lt;/code&gt; are judged as very dissimilar,
although they have the same syntactic structure and the words in them
have very similar embeddings.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/kew0g4190z5smsg/Screenshot%202018-05-01%2018.29.46.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Example sentence pairs from the STS Benchmark.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;similarity-methods&quot;&gt;Similarity Methods&lt;/h2&gt;

&lt;p&gt;There is a wide range of methods for calculating the similarity in meaning between two sentences. Here we take a look
at the most common ones.&lt;/p&gt;

&lt;h3 id=&quot;baselines&quot;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;The easiest way of estimating the semantic similarity between a pair of sentences
is by taking the average of the word embeddings of
all words in the two sentences, and calculating the cosine between the resulting embeddings. Obviously, this simple
baseline leaves considerable room for variation. We’ll investigate the effects of ignoring stopwords
and computing an average weighted by tf-idf in particular.&lt;/p&gt;

&lt;h3 id=&quot;word-movers-distance&quot;&gt;Word Mover’s Distance&lt;/h3&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/ou8hfnym922o8yj/Screenshot%202018-05-01%2018.21.33.png?raw=1&quot; /&gt;
&lt;figcaption&gt;The Word Mover&#39;s Distance between two documents is the
minimum cumulative distance that all words in document 1 need
to travel to exactly match document 2.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;One interesting alternative to our baseline is &lt;a href=&quot;http://proceedings.mlr.press/v37/kusnerb15.pdf&quot;&gt;Word Mover’s Distance&lt;/a&gt;.
WMD uses the word embeddings of the words in two texts to measure the minimum distance that the words in one
text need to “travel” in semantic space to reach the words in the other text.&lt;/p&gt;

&lt;h3 id=&quot;smooth-inverse-frequency&quot;&gt;Smooth Inverse Frequency&lt;/h3&gt;

&lt;p&gt;Taking the average of the word embeddings in a sentence tends to give too much weight to words that are
quite irrelevant, semantically speaking. &lt;a href=&quot;https://openreview.net/forum?id=SyK00v5xx&quot;&gt;Smooth Inverse Frequency&lt;/a&gt; tries to solve this problem in two ways:&lt;/p&gt;

&lt;ol class=&quot;nomargin&quot;&gt;
&lt;li&gt;Weighting: like our tf-idf baseline above, SIF takes the weighted average of the word embeddings in the sentence.
Every word embedding is
weighted by &lt;code&gt;a/(a + p(w))&lt;/code&gt;, where &lt;code&gt;a&lt;/code&gt; is a parameter that is typically set to &lt;code&gt;0.001&lt;/code&gt; and &lt;code&gt;p(w)&lt;/code&gt; is the estimated frequency
of the word in a reference corpus.&lt;/li&gt;

&lt;li&gt;Common component removal: next, SIF computes the principal component of the resulting embeddings
for a set of sentences. It then subtracts from these sentence embeddings their projections on their
first principal component. This should remove variation related to frequency and syntax that is less relevant
semantically.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a result, SIF downgrades unimportant words such as &lt;code&gt;but&lt;/code&gt;, &lt;code&gt;just&lt;/code&gt;, etc., and
keeps the information that contributes most to the semantics of the sentence.
&lt;/p&gt;

&lt;h3 id=&quot;pre-trained-encoders&quot;&gt;Pre-trained encoders&lt;/h3&gt;

&lt;p&gt;
All methods above share two important characteristics. First, as simple bag-of-word methods,
they do take not word order into account. Second, the word embeddings they use have been learned
in an unsupervised manner. Both these traits are potentially harmful. Since differences in
word order often go hand in hand with differences in meaning (compare &lt;code&gt;the dog bites the man&lt;/code&gt; with &lt;code&gt;the man bites the dog&lt;/code&gt;),
we&#39;d like our sentence embeddings to be sensitive to this variation. Additionally, supervised training
can help sentence embeddings learn the meaning of a sentence more directly.
&lt;/p&gt;

&lt;p&gt;This is where pre-trained encoders come in. Pre-trained sentence encoders aim to play the same role as word2vec
and GloVe, but for sentence embeddings: the embeddings they produce can be used in a variety of
applications, such as text classification, paraphrase detection, etc. Typically they have been trained
on a range of supervised and unsupervised tasks, in order to capture as much universal semantic
information as possible. Several such encoders are available. We&#39;ll take a look at InferSent and the Google Sentence Encoder.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/6ik0l49ad1mrj8d/Screenshot%202018-05-01%2019.09.14.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Pre-trained sentence encoders rely on tasks such as Natural Language Inference to learn sentence
embeddings that can be used in transfer tasks.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/InferSent&quot;&gt;InferSent&lt;/a&gt; is a pre-trained encoder that was developed
by Facebook Research. It is a BiLSTM with max pooling, trained on the SNLI dataset, 570k English sentence pairs labelled with one of three categories: entailment, contradiction or neutral.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder/1&quot;&gt;Google Sentence Encoder&lt;/a&gt; is Google’s answer to Facebook’s InferSent. It comes in two forms:&lt;/p&gt;

&lt;ul class=&quot;nomargin&quot;&gt;
&lt;li&gt;an advanced model that takes the element-wise sum of the context-aware word representations produced by the encoding subgraph of a Transformer model.&lt;/li&gt;
&lt;li&gt;a simpler Deep Averaging Network (DAN) where input embeddings for words and bigrams are averaged together and passed through a feed-forward deep neural network.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Transformer-based model tends to give better results, but at the time of writing, only the DAN-based encoder was available.

In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data (in a skip-thought-like task) and supervised data (the SNLI corpus).&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We tested all the methods above by getting their similarities for the sentence pairs in the development
and test sets of the SICK and STS data, and computing their correlation with the human judgements. We’ll
mostly work with Pearson correlation, as is standard in the literature, except where Spearman correlation gives different results.&lt;/p&gt;

&lt;h3 id=&quot;baselines-1&quot;&gt;Baselines&lt;/h3&gt;

&lt;p&gt;Despite their simplicity, the baseline methods that take the cosine between average word embeddings can perform
surprisingly well. Still, a few conditions have to be met:&lt;/p&gt;

&lt;ul class=&quot;nomargin&quot;&gt;
&lt;li&gt;Simple word2vec embeddings outperform GloVe embeddings.&lt;/li&gt;
&lt;li&gt;With word2vec, it is unclear whether using a stoplist or tf-idf weighting helps. On STS it sometimes does, on SICK it does not. Simply computing an unweighted average of all word2vec embeddings consistently does pretty well.&lt;/li&gt;
&lt;li&gt;With GloVe, using a stoplist is crucial to obtaining good results. Using tf-idf weights does not help, with or without a stoplist.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;/images/blog/sensim-baselines.png&quot; /&gt;
&lt;figcaption&gt;Our simple baselines can perform surprisingly well.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;word-movers-distance-1&quot;&gt;Word Mover’s Distance&lt;/h3&gt;

&lt;p&gt;Based on our results, there’s little reason to use Word Mover’s Distance rather than simple word2vec averages. Only on STS-TEST, and only in combination with a stoplist, can WMD compete with the simpler baselines.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;/images/blog/sensim-wmd.png&quot; /&gt;
&lt;figcaption&gt;Word Mover&#39;s Distance disappoints.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;smooth-inverse-frequency-1&quot;&gt;Smooth Inverse Frequency&lt;/h3&gt;

&lt;p&gt;Smooth Inverse Frequency is the most consistent performer in our tests. On the SICK data, it does about as well as its baseline competitors, on STS it outranks them by a clear margin. Note there is little difference between SIF with word2vec embeddings and SIF with GloVe embeddings. This is remarkable, given the large differences between the two we observed above. It shows SIF’s weighting and common component removal effectively reduces uninformative noise from the embeddings.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;/images/blog/sensim-sif.png&quot; /&gt;
&lt;figcaption&gt;Smooth Inverse Frequency is the most consistent performer.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;pre-trained-encoders-1&quot;&gt;Pre-trained encoders&lt;/h3&gt;

&lt;p&gt;Pre-trained encoders have a lot to be said for them. However, our results indicate they are not yet able to capitalize fully on their training regime. Google’s Sentence Encoder looks like a better choice than InferSent, but the Pearson correlation coefficient shows very little difference with Smooth Inverse Frequency.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;/images/blog/sensim-enc-pearson.png&quot; /&gt;
&lt;figcaption&gt;Pre-trained encoders perform well, but SIF gives them a run for their money.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The differences in Spearman correlation are more outspoken. This may indicate that the Google Sentence Encoder more often gets the ordering of the sentences right, but not necessarily the relative differences between them.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;/images/blog/sensim-enc-spearman.png&quot; /&gt;
&lt;figcaption&gt;Spearman correlation shows a larger difference than Pearson correlation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Sentence similarity is a complex phenomenon. The meaning of a sentence does not only depend on the words
in it, but also on the way they are combined. As the harp-keyboard example above shows, semantic
similarity can have several dimensions, and sentences may be similar in one but opposite in the other.
Current sentence embedding methods only scratch the surface of what’s possible.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;/images/blog/sensim-all.png&quot; /&gt;
&lt;figcaption&gt;The combined results of all sentence similarity methods.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;So, what should you do when you’re looking to compute sentence similarities? Our results suggest the following:&lt;/p&gt;

&lt;ul class=&quot;nomargin&quot;&gt;
&lt;li&gt;Word2vec embeddings are a safer choice than GloVe embeddings.&lt;/li&gt;
&lt;li&gt;Although an unweighted average of the word embeddings in the sentence holds its own as a simple baseline, Smooth Inverse Frequency is usually a stronger alternative.&lt;/li&gt;
&lt;li&gt;If you can use a pre-trained encoder, pick Google&#39;s Sentence Encoder, but remember its performance gain may not be all that spectacular.&lt;/li&gt;
&lt;/ul&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/sentence-similarity/&quot;&gt;Comparing Sentence Similarity Methods&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on May 02, 2018.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Why computers don’t yet read better than us]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/why-computers-dont-read-better-than-us/" />
  <id>http://www.nlp.town/blog/why-computers-dont-read-better-than-us</id>
  <published>2018-01-31T07:00:00-05:00</published>
  <updated>2018-01-31T07:00:00-05:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
Artificial Intelligence is on a roll these days. It feels like the media report a new breakthrough every day. 
In 2017, &lt;a href=&quot;https://newatlas.com/ai-2017-beating-humans-games/52741/&quot;&gt;computer and board games&lt;/a&gt; were at 
the center of public attention, but this year things look different. In the early days of 2018, both Microsoft 
and Alibaba claimed to have developed software that can read as well as humans do. Sensational headlines followed 
suit. CNN wrote that “&lt;a href=&quot;http://money.cnn.com/2018/01/15/technology/reading-robot-alibaba-microsoft-stanford/index.html&quot;&gt;Computers are getting better than humans at reading&lt;/a&gt;”, while Newsweek feared “&lt;a href=&quot;http://money.cnn.com/2018/01/15/technology/reading-robot-alibaba-microsoft-stanford/index.html&quot;&gt;Computers can now read better than humans, putting millions 
of jobs at risk&lt;/a&gt;”. Reality is much less spectacular, however.
&lt;/p&gt;

&lt;h2 id=&quot;squad&quot;&gt;SQuAD&lt;/h2&gt;

&lt;p&gt;Every AI challenge needs a fancy title. The catalyst behind the new breakthrough in Question Answering (QA) is 
SQuAD, the &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;Stanford Question Answering Dataset&lt;/a&gt;. SQuAD brings together over 100,000 questions and answers about hundreds of Wikipedia articles. For example, there is a series of questions about the Oil Crisis, such as &lt;em&gt;When did the 1973 oil crisis begin?&lt;/em&gt; (October 1973) or &lt;em&gt;What was the price of oil in March of 1974?&lt;/em&gt; (&amp;dollar;12). The idea is that researchers use 80&amp;#37; of these questions to train their QA models. During this training process, their software can learn how to find the answers in the texts and pick up the difference between various question words such as &lt;em&gt;where&lt;/em&gt; and &lt;em&gt;when&lt;/em&gt;. Afterwards, it is evaluated on the remaining 20&amp;#37; of the questions it has never seen before.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/88iftblw9rwv5ja/Screenshot%202018-01-17%2014.04.55.png?raw=1&quot; /&gt;
&lt;figcaption&gt;An example Wikipedia paragraph from SQuAD, with the corresponding questions and answers.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;SQuAD has quickly become an influential dataset in Natural Language Processing, 
as it allows AI researchers to evaluate their software objectively and to compare their systems with one another. It’s no coincidence that since its release in 2016, NLP
has seen increasing interest in the development of QA systems. Still, we shouldn’t overestimate the progress that has been made. It is actually much easier for a piece of software to score well on the Wikipedia questions in SQuAD than you would think.&lt;/p&gt;

&lt;p&gt;First of all, the SQuAD challenge is really not that difficult. Together with each question, participating
QA systems are presented with the Wikipedia paragraph that is guaranteed to
contain the answer. This simplifies the challenge considerably: instead of fully interpreting a paragraph, the task boils down
to identifying the words that most likely form the answer to the question. In many cases, this is pretty straightforward.
When the question starts with &lt;em&gt;where&lt;/em&gt; and the paragraph contains only one location, for example, 
little can go wrong. Then there’s the fact that the participating QA systems don’t have to search Wikipedia for the relevant paragraphs. There are &lt;a href=&quot;https://arxiv.org/pdf/1704.00051.pdf&quot;&gt;systems that do this&lt;/a&gt;, but they score much worse on the SQuAD test.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/sads42mxjiqsof8/Screenshot%202018-01-17%2022.11.07.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Right now, two QA systems share the first place on the SQuAD scoreboard.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Second, the human score on SQuAD &amp;mdash; 82.3&amp;#37; correct answers &amp;mdash; is undoubtedly an underestimation of reality. 
The human answers were collected via &lt;a href=&quot;https://www.mturk.com/&quot;&gt;Amazon Mechanical Turk&lt;/a&gt; &amp;mdash; a site where people solve simple tasks against payment. 
Because they earn very little for these tasks, so-called “Turkers” often work quickly and sloppily. What’s more, 
most of their “mistakes” are not really incorrect answers. Usually these are cases where one Turker’s answer happens to contain 
one or two words more or fewer than those of his colleagues. If one person chooses &lt;em&gt;nearly &amp;dollar;12&lt;/em&gt; to answer the oil 
price question above, and everyone else selects &lt;em&gt;&amp;dollar;12&lt;/em&gt;, the first answer is considered incorrect. In 
contrast to an uninformed Turker, the competing QA 
software knows how Turkers typically select their answers: it has seen thousands 
of examples during its training process.&lt;/p&gt;

&lt;h2 id=&quot;question-answering&quot;&gt;Question Answering&lt;/h2&gt;

&lt;p&gt;Admittedly, even if the current QA systems do not outperform people, they still score an impressive 83&amp;#37; on a reading test &amp;mdash; a feat that initially may look like a proof of intelligence. However, we shouldn’t attribute too much intelligence to the software. Although there are of course individual differences, modern QA systems such as Microsoft’s and Alibaba’s are hardly able to really interpret a text. Instead they rely on pattern matching &amp;mdash; very sophisticated pattern matching, but that’s all there is to it.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/gd7ebu1phpxufxi/Screenshot%202018-01-17%2014.10.50.png?raw=1&quot; /&gt;
&lt;figcaption&gt;The Wikipedia paragraph about the oil crisis from SQuAD, with the answers from the Alibaba model.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As they look for the answer to a question, modern QA systems first try to find a mapping between the question and the Wikipedia paragraph they are presented with. For a question like &lt;em&gt;When did the oil crisis of 1973 begin?&lt;/em&gt; this is not very difficult: by searching for the words &lt;em&gt;1973&lt;/em&gt;, &lt;em&gt;oil crisis&lt;/em&gt; and &lt;em&gt;begin&lt;/em&gt;, the software quickly finds the correct sentence in the paragraph above. The question word &lt;em&gt;when&lt;/em&gt; provides the final piece of the puzzle: from the training data, the software has been able to learn that &lt;em&gt;when&lt;/em&gt;-questions always take a point in time as an answer. It therefore selects the only time in the relevant sentence (October 1973), and answers the question correctly.&lt;/p&gt;

&lt;p&gt;The second question in the example above (What was the price of oil in March 1974?) shows the limitations of this method. 
By looking for the words &lt;em&gt;price&lt;/em&gt;, &lt;em&gt;oil&lt;/em&gt;, &lt;em&gt;March&lt;/em&gt; and &lt;em&gt;1974&lt;/em&gt; the software still finds the right sentence, but interestingly 
enough, this sentence contains two possible prices: &lt;em&gt;US&amp;dollar;3 per barrel&lt;/em&gt; and &lt;em&gt;&amp;dollar;12&lt;/em&gt;. Because it doesn’t 
interpret the sentence but only matches patterns to each other, the QA system now selects the first price it sees: &lt;em&gt;US&amp;dollar;3 per barrel&lt;/em&gt;. 
The SQuAD page shows that both the Microsoft and the Alibaba systems come up with this incorrect answer. Indeed, this lack of interpretation is a fundamental shortcoming of all current QA systems.&lt;/p&gt;

&lt;h1 id=&quot;one-trick-ponies&quot;&gt;One-trick ponies&lt;/h1&gt;

&lt;p&gt;Most existing AI systems are one-trick ponies. If they have been trained on one particular task and one particular text type, they usually cannot handle different domains or problems. QA software that has been trained on Wikipedia will mostly fail to answer questions about other texts, such as legal documents or scientific articles. To do this, it would need to see tens of thousands of questions and answers from that particular domain. Collecting such training data is pretty expensive at best, and often it is an insurmountable task.&lt;/p&gt;

&lt;p&gt;The tunnel vision of the current QA systems is even worse than that. Last summer, two researchers from Stanford University demonstrated &lt;a href=&quot;https://arxiv.org/pdf/1707.07328.pdf&quot;&gt;how easy it is to fool QA software&lt;/a&gt; trained on SQuAD: by changing just a few details in the Wikipedia texts, they managed to bring down the quality of the best systems enormously.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/crqzr3oxft2n9go/Screenshot%202018-01-17%2021.59.52.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Add an additional sentence with a possible answer, and the QA software starts to guess.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The oil price example already demonstrated the main weakness of current QA software: if the Wikipedia paragraph contains several possible answers, even the best QA systems begin to guess. The same problem is illustrated by the Super Bowl paragraph above: if you simply add an extra quarterback to the text, the systems can’t tell anymore which player was exactly 38 years old during the 33rd Super Bowl. Worse still, if you add to the article an ungrammatical sequence of words that are vaguely related to the answer, even the best systems fall below 10&amp;#37; correct answers. People deal with such misleading situations much better.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;One thing is clear: we’re still a long way from computers that read as well as humans. Still, the recent evolution in QA systems is quite promising &amp;mdash; or frightening, if you fear job loss in the long term. After all, how often are we not confronted with unstructured collections of text? Legal documents, scientific literature, or even the mails of Hillary Clinton &amp;mdash; all hundreds or thousands of pages with interesting content that no one can read from A to Z. Wouldn’t it be great if QA software could answer all our questions about them?&lt;/p&gt;

&lt;p&gt;However, the real breakthrough will only come when QA systems lose their dependence on expensive training data. When they can answer a question about a new domain without first seeing tens of thousands of similar examples. The success of such so-called “unsupervised” methods will undoubtedly herald a new revolution in Artificial Intelligence. But we’re not quite there yet.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/why-computers-dont-read-better-than-us/&quot;&gt;Why computers don’t yet read better than us&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on January 31, 2018.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Named Entity Recognition and the Road to Deep Learning]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/ner-and-the-road-to-deep-learning/" />
  <id>http://www.nlp.town/blog/ner-and-the-road-to-deep-learning</id>
  <published>2017-09-12T08:00:00-04:00</published>
  <updated>2017-09-12T08:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
Not so very long ago, Natural Language Processing looked very different. In sequence labelling tasks such as 
Named Entity Recognition, Conditional Random Fields were the go-to model. The main challenge for NLP
engineers consisted in finding good features that captured their data well. Today, deep learning has replaced
 CRFs at the forefront of sequence labelling, and the focus has shifted from feature engineering to designing and implementing effective neural network
architectures. Still, the old and the new-style NLP are not diametrically opposed: just as it is possible
(and useful!) to incorporate neural-network features into a CRF, CRFs have influenced some of the best 
deep learning models for sequence labelling.
&lt;/p&gt;

&lt;p&gt;
One of the most popular sequence labelling tasks is &lt;a href=&quot;http://nlp.town/blog/nlp-api-ner/&quot;&gt;Named Entity Recognition&lt;/a&gt;, 
where the goal is to identify the names of entities in a sentence.
Named entities can be generic proper nouns that refer to locations, people or organizations,
but they can also be much more domain-specific, such as diseases or genes in biomedical NLP. A seminal task for Named Entity
Recognition was the &lt;a href=&quot;https://www.clips.uantwerpen.be/conll2003/ner/&quot;&gt;CoNLL-2003 shared task&lt;/a&gt;, 
whose training, development and testing data are still often used
to compare the performance of different NER systems. In this blog post, we’ll rely on this data to help us answer a few questions
about how the standard approach to NER has evolved in the past few years: are neural networks really better than CRFs?
What components make up an effective deep learning architecture for NER? And in what way can CRFs and neural networks be combined?
&lt;/p&gt;

&lt;p&gt;
For a long time, Conditional Random Fields were the standard model for sequence labelling tasks. CRFs take as their
input a set of features for each token in a sentence, and learn to predict an optimal sequence
of labels for the full sentence. When you develop a CRF, a lot of time and effort typically goes into finding those
feature functions that help the model most at predicting the likelihood of every label for a given word.
Several software libraries exist for developing CRFs: &lt;a href=&quot;http://mallet.cs.umass.edu/&quot;&gt;Mallet&lt;/a&gt;, 
&lt;a href=&quot;http://www.chokkan.org/software/crfsuite/&quot;&gt;CRFSuite&lt;/a&gt; and &lt;a href=&quot;https://taku910.github.io/crfpp/&quot;&gt;CRF++&lt;/a&gt; are some
of the most popular ones.
For this article, I used CRF++ to train a vanilla CRF with some common features for Named Entity Recognition: the token itself, its bigram and trigram prefix and suffix, its part of 
speech, its chunk type, several pieces of information about the word’s form (Does it start with a capital? Is it uppercase? Is it a digit?),
and a subset of those features for the surrounding context words. Out of the box, this baseline CRF obtains an F1-score of 81.15% on the b test set
of the CONLL-2003 data. 
This puts it in thirteenth place in the original CONLL ranking from 2003. That’s not great, but given
how quickly we can get this result, it’s not too shabby either. 
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/jrx7a906e9ysyek/Screenshot%202017-09-11%2014.46.47.png?raw=1&quot; /&gt;
&lt;figcaption&gt;The relationship between CRFs, HMMs, Logistic Regression and Naive Bayes. From Sutton &amp;amp; McCallum’s &lt;a href=&quot;http://www.research.ed.ac.uk/portal/files/10482724/crftut_fnt.pdf&quot;&gt;An Introduction to Conditional Random Fields&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
How can we improve this performance? One of the most well-known weaknesses of the previous generation of NLP models is that 
they are unable to model semantic similarity between two words. For all our CRF knows,
&lt;em&gt;Paris&lt;/em&gt; and &lt;em&gt;London&lt;/em&gt; differ as much in meaning as &lt;em&gt;Paris&lt;/em&gt; and &lt;em&gt;cat&lt;/em&gt;. To overcome this problem, many CRFs for 
Named Entity Recognition rely on gazetteers &amp;mdash; lists with names of people, locations and organizations that are known in advance. 
This is a suboptimal solution, however: gazetteers are inherently limited and can be costly to develop. Also, they’re usually only available for the 
named entities themselves, so that the semantic similarity between other words in their context goes unnoticed. This is 
where it can pay off to take the first step on the path to deep learning.&lt;/p&gt;

&lt;p&gt;
As I’ve &lt;a href=&quot;http://nlp.yvespeirsman.be/blog/anything2vec/&quot;&gt;written before&lt;/a&gt;, word embeddings are 
one of the main drivers behind the success of deep learning in NLP. The first layer in a neural network
for text processing is mostly an embedding layer that maps one-hot word vectors to their dense 
embeddings. Still, this does not mean that the distributional information in word embeddings can only be leveraged 
in neural networks &amp;mdash; it’s also possible to feed this information to a CRF. One popular way of doing this is to 
cluster a set of word embeddings by distributional similarity, and provide the CRF with the cluster IDs of a token and its context words. 
Following the recommendations of &lt;a href=&quot;https://arxiv.org/pdf/1705.01265.pdf&quot;&gt;Balikas et al. (2017)&lt;/a&gt;, I 
trained a set of 64-dimensional word embeddings on the English Wikipedia for all words that
appear at least 1000 times in that corpus. I then used K-means to group these embeddings in 500 clusters.
While it’s easy enough to write the code for this with the help of libraries such as &lt;a href=&quot;https://radimrehurek.com/gensim/&quot;&gt;Gensim&lt;/a&gt;
or &lt;a href=&quot;http://scikit-learn.org/&quot;&gt;Scikit-learn&lt;/a&gt;, &lt;a href=&quot;https://github.com/dav/word2vec&quot;&gt;Google’s original word2vec code&lt;/a&gt; actually has
a command that lets us perform all the necessary steps in one go. You can find it in the bash script &lt;code&gt;demo-word.sh&lt;/code&gt;.
&lt;p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/umrhocphinsh518/Screenshot%202017-09-11%2015.04.12.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Word embeddings contain useful information for Named Entity Recognition. From &lt;a href=&quot;https://colah.github.io/posts/2015-01-Visualizing-Representations/&quot;&gt;Christopher Olah&#39;s blog&lt;/a&gt;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
The clusters we obtain are a treasure trove for Named Entity Recognition.
For example, cluster 437 contains many location names, such as &lt;em&gt;München&lt;/em&gt;, &lt;em&gt;Paris&lt;/em&gt;
and &lt;em&gt;Brussels&lt;/em&gt;. The presence of a target word in this cluster clearly increases the 
probability that it refers to a location. Similarly, cluster 254 contains verbs such as 
&lt;em&gt;say&lt;/em&gt;, &lt;em&gt;think&lt;/em&gt; or &lt;em&gt;pray&lt;/em&gt;. When a target word precedes one of the words
from this cluster, this may indicate that it refers to a person.
This intuition is backed up by the results of my simple experiment: when we add the cluster
IDs of these 500 clusters as features to our simple CRF, its F1-score on the named
entities increases from 81.15% to 84.86%. That puts it somewhere in the middle of the 2003 CONLL 
rankings. Adding additional features, such as
gazetteers, or informative feature conjuctions, is certain to take up the F1-score
further still.&lt;/p&gt;

&lt;p&gt;
Useful as these embedding clusters may be, there is of course much more to neural networks than shallow word embedding
models. Let’s therefore leave our CRF behind and build a completely neural model to see how much improvement deep learning can bring.
As so often in NLP, our model starts with a simple &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;LSTM&lt;/a&gt;. 
LSTMs are a type of &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;Recurrent Neural Network&lt;/a&gt;, which means that they are 
ideal for dealing with sequential data. The great thing about LSTMs is that they are able to learn long-term dependencies, as
their structure allows the network to pass or block information from one time step to the other.
Thanks to software libraries such as &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;Tensorflow&lt;/a&gt;,
building an LSTM has become pretty straightforward. First we need a so-called embedding layer, which maps the input words to dense word embeddings.
Then we add the LSTM layer that reads these word embeddings and outputs a new vector at every step. Finally, a dense layer maps these output
vectors to a n-dimensional vector of logits, where &lt;em&gt;n&lt;/em&gt; is the number of output labels, and a softmax function turns the logits
into label probabilities. This vanilla neural network is a great introduction to deep learning, but unfortunately it is far from effective. Where our CRF above easily obtained 
F1-scores of above 80%, you’ll be hard pressed to get above 70% with this simple LSTM.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/o6d7uncab5s5aec/Screenshot%202017-09-12%2011.16.36.png?raw=1&quot; /&gt;
&lt;figcaption&gt;A simple LSTM-based neural network for sequence labelling.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
The most glaring problem is that our standard LSTM only has access to the left context of a word when it assigns a label.
Our CRF, by contrast, took both left and right context into account. Luckily there’s a simple solution to this problem:
bidirectional LSTMs. BiLSTMs process the data with two separate LSTM layers. One of them reads the 
sentence from left to right, while the other one receives a reversed copy of the sentence to read it from
right to left. Both layers produce an output vector for every single word, and these two output vectors are concatenated
to a single vector that now models both the right and the left context. Replacing the simple LSTM layer in
our model by a biLSTM has an enormous impact on its performance. With a few standard settings &amp;mdash; a dimensionality of 300 for the
word embeddings and the LSTM, a dropout of 50% after the embedding and the LSTM layer, a batch size of 32, optimization with Adam,
 an initial learning rate of 0.001 and learning rate decay of 95% &amp;mdash; 
 its F1-score goes up from 64.9% to 76.1%. This is a
big leap forward, but we’re still far below the performance of our CRF.
&lt;/p&gt;

&lt;p&gt;
Apart from the surrounding context, there’s another area where our current neural network is at a disadvantage to
our earlier model. Whereas the word embeddings that we clustered for our CRF were trained on all of
Wikipedia, the CoNLL training data contains only around 200,000 words. This is not nearly enough to train reliable
word embeddings. To combat this data sparsity, it is common to initialize the embedding layer in a neural network with word embeddings
that were trained on another, much
larger corpus. In our experiments, we’ll use the 300-dimensional &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe&lt;/a&gt;
vectors. Relying on these pre-trained embeddings rather than letting our network learn its own embeddings
finally makes it competitive with our earlier CRF: we now obtain an F1-score of 85.9%.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/qf1hislhi8lt0d1/Screenshot%202017-09-11%2014.39.41.png?raw=1&quot; /&gt;
&lt;figcaption&gt;The Tensorboard graph for our final model.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
Despite this leap in performance, our current neural network is still handicapped compared to our CRF.
Remember that we informed the CRF about the prefixes and suffixes of the tokens in the sentence. Many entities
share informative suffixes, such as the frequent &lt;em&gt;-land&lt;/em&gt; or &lt;em&gt;-stan&lt;/em&gt; for country names. Because 
it treats all tokens as atomic entities, our neural network currently has no access to this type of information.
This blindness has given rise to the recent popularity of character-based models in NLP. As these models look
at the individual characters of a word, they can collect important information present in the character sequences. Character-based networks tend 
to take one of two forms: Recurrent Neural Networks such as LSTMs or GRUs, or 
&lt;a href=&quot;http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/&quot;&gt;Convolutional Neural Networks&lt;/a&gt;. 
The former build a vector representation for every token by reading it character by character and “remembering”
important information. The latter have a number of filters that treat tokens as chunks of character n-grams, and
build a character-based representation by combining the outputs of these filters in one vector.
These character-based vectors can then be concatenated to our earlier word embeddings to obtain two complementary views of the tokens. 
It is these concatenated vectors that are now fed to the BiLSTM we built earlier. In our
experiments, both methods achieved fairly similar results, but the character-based biLSTM edged out the CNN by a small
margin. As a result of this new ability to draw information from the inner structure of a word,
our neural network now outperforms the CRF quite clearly: its F1-score goes up to 89.3%. This means we’ve left the 2003
CONLL rankings behind us.&lt;/p&gt;

&lt;p&gt;
But who can stop when an F1-score of 90% is so tantalizingly close? A final disadvantage of the current method that we’ll discuss here, is that it predicts 
all labels independently of each other: as it labels a word in a sentence, it does not 
take into account the labels it predicts for the surrounding words. This
can be useful information, however: a word is much more likely to carry the label I-PER when 
the previous word had the label B-PER, for example. As &lt;a href=&quot;https://arxiv.org/pdf/1508.01991.pdf&quot;&gt;Huang et al. (2016)&lt;/a&gt; suggest,
we can solve this problem by adding a Conditional Random Field layer to our network. Like
the CRF model we started out with, this CRF layer 
outputs a matrix of transition scores between two states, and dynamic programming
can help us find the optimal tag sequence for the sentence. This final addition leaves us with an F1-score
of 90.3%, about 1% below the current state of the art for this task, and a general feeling of satisfaction .
If you&#39;d like to experiment around
with this final model yourself, I recommend taking a look at the code by &lt;a href=&quot;https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf&quot;&gt;
the Ubiquitous Knowledge Processing Lab&lt;/a&gt; or &lt;a href=&quot;https://github.com/guillaumegenthial/sequence_tagging&quot;&gt;Guillaume Genthial&lt;/a&gt;, who
also has an excellent 
&lt;a href=&quot;https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html&quot;&gt;blog post&lt;/a&gt; on the subject.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/qht64zx5fyi1mun/Screenshot%202017-09-24%2023.29.44.png?raw=1&quot; /&gt;
&lt;figcaption&gt;As our models gradually become more complex, their F1-score also goes up.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
Countless experiments have shown that neural networks tend to outperform 
CRFs for the task of Named Entity Recognition on a variety of data. Still, you shouldn’t expect 
this improvement to come without effort. As you
move from a CRF or similar model to a deep learning-based solution, the time you used to spend devising good features
will now typically go to designing an effective network and tuning its many 
parameters. Before you take the plunge, it can therefore be worthwhile to investigate whether
word embeddings can improve your CRF. In the end, however, the investment in building a deep learning
approach is likely to pay off, as you’ll have a superior model that can also be applied more easily
to related tasks.
&lt;/p&gt;

&lt;/p&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/ner-and-the-road-to-deep-learning/&quot;&gt;Named Entity Recognition and the Road to Deep Learning&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on September 12, 2017.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Perplexed by Game of Thrones. A Song of N-Grams and Language Models]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/song-of-ngrams-and-lms/" />
  <id>http://www.nlp.town/blog/song-of-ngrams-and-lms</id>
  <published>2017-07-05T08:00:00-04:00</published>
  <updated>2017-07-05T08:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
N-grams have long been part of the arsenal of every NLPer. These fixed-length word sequences are not only ubiquitous 
as features in NLP tasks such as text classification, but also formed the basis of the language models underlying machine translation 
and speech recognition. However, with the advent of recurrent neural networks and the diminishing role of feature selection in 
deep learning, their omnipresence could quickly become a thing of the past. Are we witnessing the end of n-grams in Natural Language Processing?
&lt;/p&gt;

&lt;p&gt;
N-grams are sequences of &lt;em&gt;n&lt;/em&gt; linguistic items, usually words. Depending on the length of these sequences, we call them 
unigrams, bigrams, trigrams, four-grams, five-grams, etc. N-grams have often been essential in developing high-quality 
NLP applications. Traditional models for sentiment analysis, 
for example, benefit immensely from n-gram features. To classify a sentence such 
as &lt;em&gt;I did not like this movie&lt;/em&gt; as negative, it’s not sufficient to know that it contains the words &lt;em&gt;not&lt;/em&gt; and &lt;em&gt;like&lt;/em&gt; 
&amp;mdash; after all, so does the positive sentence &lt;em&gt;What’s not to like about this movie&lt;/em&gt;?
A sentiment model will have a much better chance of guessing the positive or negative nature of a sentence correctly when it relies on
 the bigrams or trigrams, such as &lt;em&gt;did not like&lt;/em&gt;, in addition to the individual words. 
&lt;/p&gt;

&lt;p&gt;This benefit of n-grams is apparent in many other examples of text classification as well.
To illustrate this, let’s do a fun experiment. Let’s take a look at the television series &lt;em&gt;A Game of Thrones&lt;/em&gt;, 
and investigate whether we can use the dialogues in the individual episodes to determine which book from George
R. R. Martin’s book series &lt;em&gt;A Song of Ice and Fire&lt;/em&gt; they are based on. 
According to Wikipedia, the episodes in series 1 are all based on the first novel, &lt;em&gt;A Game of Thrones&lt;/em&gt;,
series 2 covers the events in &lt;em&gt;A Clash of Kings&lt;/em&gt;, series 3 and 4 are adapted from A &lt;em&gt;Storm of Swords&lt;/em&gt;, and series 5 and
6 mix events from &lt;em&gt;A Feast for Crows&lt;/em&gt; and &lt;em&gt;A Dance of Dragons&lt;/em&gt;. It turns out we can induce this mapping 
pretty well on the basis of the dialogues in the episodes.&lt;/p&gt;

&lt;div class=&quot;g-py-30 text-center&quot;&gt;
&lt;figure&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://www.gstatic.com/charts/loader.js&quot;&gt;&lt;/script&gt;
&lt;div id=&quot;sankey_basic&quot; style=&quot;width: 900px; height: 300px;&quot;&gt;&lt;/div&gt;
&lt;script async=&quot;&quot; type=&quot;text/javascript&quot;&gt;
      google.charts.load(&#39;current&#39;, {
        &#39;packages&#39;: [&#39;sankey&#39;]
      });
      google.charts.setOnLoadCallback(drawChart);

      function drawChart() {
        var data = new google.visualization.DataTable();
        data.addColumn(&#39;string&#39;, &#39;From&#39;);
        data.addColumn(&#39;string&#39;, &#39;To&#39;);
        data.addColumn(&#39;number&#39;, &#39;Weight&#39;);
        data.addRows([
          [&#39;Series 1&#39;, &#39;A Game of Thrones&#39;, 10],
          [&#39;Series 2&#39;, &#39;A Clash of Kings&#39;, 10],
          [&#39;Series 3&#39;, &#39;A Storm of Swords&#39;, 1],
          [&#39;Series 3&#39;, &#39;A Storm of Swords&#39;, 9],
          [&#39;Series 4&#39;, &#39;A Storm of Swords&#39;, 9],
          [&#39;Series 4&#39;, &#39;A Feast for Crows&#39;, 0],
          [&#39;Series 4&#39;, &#39;A Dance with Dragons&#39;, 1],
          [&#39;Series 5&#39;, &#39;A Storm of Swords&#39;, 2],
          [&#39;Series 5&#39;, &#39;A Feast for Crows&#39;, 3],
          [&#39;Series 5&#39;, &#39;A Dance with Dragons&#39;, 5],
          [&#39;Series 6&#39;, &#39;A Feast for Crows&#39;, 2],
          [&#39;Series 6&#39;, &#39;A Clash of Kings&#39;, 1],
          [&#39;Series 6&#39;, &#39;A Storm of Swords&#39;, 3],
          [&#39;Series 6&#39;, &#39;A Dance with Dragons&#39;, 4]
        ]);

        // Sets chart options.
        var options = {
          width: 600,
          sankey: {
            iterations: 0,
          }
        };

        // Instantiates and draws our chart, passing in some options.
        var chart = new google.visualization.Sankey(document.getElementById(&#39;sankey_basic&#39;));
        chart.draw(data, options);
      }
&lt;/script&gt;
&lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For each episode and book, I collected the n-grams in a feature vector. I used Tf-Idf to weight the n-gram
frequencies and applied L2-normalization. For each episode, I then picked the book whose feature vector had the
highest cosine similarity with the episode’s vector. If we look at just the unigrams, 46 of the 60 episodes (76.67%)
are assigned to the correct book, according to the Wikipedia mapping. If we also take bigrams into account,
this figure climbs to 50 (83.33%). Adding trigrams takes us to 52 correct episodes (86.67%). This indicates
that the n-grams capture the similarities between the books and the episodes very well: random guessing would have only given
us a success ratio of 22%. The diagram above, which plots the mappings of the model with unigrams, bigrams and trigrams,
also suggests another interesting pattern: the later the television series, the more difficult it becomes to connect its episodes
to the correct book. While this is obviously just a toy example, this trend is in line with
&lt;a href=&quot;https://www.pastemagazine.com/articles/2015/05/20-big-differences-between-the-game-of-thrones-tv.html&quot;&gt;observations&lt;/a&gt;
that with each &lt;em&gt;Game of Thrones&lt;/em&gt; series, the screen adaptation has become &lt;a href=&quot;http://www.vanityfair.com/hollywood/2016/06/game-of-thrones-season-6-debate&quot;&gt;
less faithful to the books&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;Apart from text classification, n-grams are most often used in the context of language models. Language models are models that can assign a probability
to a sequence of words, such as a sentence. This ability underpins many successful NLP applications, such as speech recognition, 
machine translation and text generation. In speech recognition, say, it is useful to know that the sentence &lt;em&gt;there is a hair in my soup&lt;/em&gt;
is much more probable than &lt;em&gt;there is a air in my soup&lt;/em&gt;. Traditionally, language models estimated these probabilities on the 
basis of the frequencies of the n-grams they had observed in a large corpus. A bigram model would 
look up the frequencies of the relevant bigrams, such as &lt;em&gt;a hair&lt;/em&gt; and &lt;em&gt;a air&lt;/em&gt;, in 
its training corpus, while a trigram model would rely on the frequencies of the three-word sequences,
such as &lt;em&gt;is a hair&lt;/em&gt;. As the figures from the Google Books N-gram corpus below illustrate, 
these frequencies should normally
point towards a large preference for &lt;em&gt;there is a hair in my soup&lt;/em&gt;. Even when the speaker does not
pronounce the first letter in &lt;em&gt;hair&lt;/em&gt;, the language model will help ensure that the speech recognition
system gets the sentence right.
&lt;/p&gt;

&lt;iframe src=&quot;https://books.google.com/ngrams/interactive_chart?content=is+a+hair%2C+is+a+air&amp;amp;year_start=1800&amp;amp;year_end=2000&amp;amp;corpus=15&amp;amp;smoothing=3&amp;amp;share=&amp;amp;direct_url=t1%3B%2Cis%20a%20hair%3B%2Cc0%3B.t1%3B%2Cis%20a%20air%3B%2Cc0&quot; width=&quot;600&quot; height=&quot;275&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; hspace=&quot;0&quot; vspace=&quot;0&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;
The predictions of language models are usually expressed in terms of &lt;em&gt;perplexity&lt;/em&gt;. The perplexity of a particular model on a
sentence is the inverse of the probability it assigns to that sentence, normalized for sentence length. Intuitively, the perplexity expresses
how confused the model is by the words in the sentence: a perplexity of 50 means that the model behaves
as if it were randomly picking a word from 50 candidates at each step. 
The lower the perplexity, the better the language model is at predicting the text.&lt;/p&gt;

&lt;p&gt;Let’s return for a minute to the increasing difficulty of linking the &lt;em&gt;Game of Thrones&lt;/em&gt; episodes in the later 
series to the books.
If these newer episodes are indeed less faithful to their source material than earlier ones, this should be reflected in a rising perplexity of a language model trained
on the books. To verify this, I trained a bunch of language models on all books in the &lt;em&gt;Song of Ice and Fire&lt;/em&gt; series, using the KenLM toolkit.
&lt;a href=&quot;https://kheafield.com/papers/avenue/kenlm.pdf&quot;&gt;KenLM&lt;/a&gt; implements n-gram language models that are equipped with some advanced smoothing
techniques to deal with word sequences they have never seen in the training corpus. 
The results in the figure below confirm our expectations: the median perplexity of the four-gram 
language model on the sentences in the episodes climbs with every series. While on average,
the median sentence perplexity in the first series is 38, it increases to 48 in the second and
third series, 52 in the fourth series, 56 in the fifth and 57 in the sixth. This
pattern is repeated by the other language models I trained (from bigram to 5-gram models). Clearly, n-gram language models
have a harder time predicting the dialogues in the later episodes from the language in the books.
&lt;/p&gt;

&lt;iframe width=&quot;700&quot; height=&quot;370&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1Tir_tqVJLQIr9nY868y1dYAwPgM33qSRQbMzR6jFkgU/pubchart?oid=1201461159&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;
As with so many traditional NLP approaches, in recent years n-gram based language models have been outshone by their neural 
counterparts. Because neural networks rely on distributed representations that capture the meaning of words 
(the so-called word embeddings), they are much better at dealing 
with unseen word sequences. For example, an n-gram model may not be able to predict that &lt;em&gt;pitcher of beer&lt;/em&gt; is more likely 
than &lt;em&gt;pitcher of cats&lt;/em&gt; if it has never seen these trigrams in the training corpus. This is true even when the training corpus 
contains many examples of &lt;em&gt;glass of beer&lt;/em&gt; or &lt;em&gt;bottle of beer&lt;/em&gt;, but no examples of &lt;em&gt;glass of cats&lt;/em&gt; or &lt;em&gt;bottle of cats&lt;/em&gt;. 
A neural language model, by contrast, will typically have learnt that &lt;em&gt;pitcher&lt;/em&gt;, &lt;em&gt;glass&lt;/em&gt; and &lt;em&gt;bottle&lt;/em&gt; have similar meanings, 
and it will use this information to make the right prediction. These dense word embeddings also have a convenient side effect: neural language models 
take up much less memory than n-gram models.
&lt;/p&gt;

&lt;p&gt;
The first neural language models were still heavily indebted to their predecessors, as they applied neural networks to n-grams.
&lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;Bengio et al.’s (2003)&lt;/a&gt; seminal approach learnt word embeddings for all words in the n-gram, along with the probability of the word sequence. Later models, 
such as those by &lt;a href=&quot;http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf&quot;&gt;Mikolov et al. (2010)&lt;/a&gt; 
and &lt;a href=&quot;http://www.quaero.org/media/files/bibliographie/sundermeyer_lstm_neural_interspeech2012.pdf&quot;&gt;Sundermeyer et al. (2013)&lt;/a&gt; 
did away with the n-grams altogether and introduced recurrent neural networks (RNNs) to the task of language modelling. These RNNs, and Long Short-Term Memories in particular,
not only learn word embeddings, but are also able to model dependencies that exceed the boundaries of traditional n-grams.
Even more recently, other neural architectures have been applied to language modelling, such as 
&lt;a href=&quot;https://arxiv.org/pdf/1612.08083.pdf&quot;&gt;convolutional networks&lt;/a&gt; and 
&lt;a href=&quot;https://arxiv.org/pdf/1702.04521.pdf&quot;&gt;RNNs with attention&lt;/a&gt;. Teams at 
&lt;a href=&quot;https://arxiv.org/pdf/1602.02410.pdf&quot;&gt;Google&lt;/a&gt;, 
&lt;a href=&quot;https://research.fb.com/building-an-efficient-neural-language-model-over-a-billion-words/&quot;&gt;Facebook&lt;/a&gt; 
and other places seem to have got 
caught up in a race for ever-lower perplexities. 
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/5n4dn24x1vcb4ag/Screenshot%202017-07-04%2023.30.05.png?raw=1&quot; /&gt;
&lt;figcaption&gt;RNN-based language models learn word embeddings to make predictions. Adapted from the Stanford CS224n course slides.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are quite a few toolkits for training and testing neural language models. &lt;a href=&quot;https://nlg.isi.edu/software/nplm/&quot;&gt;NPLM&lt;/a&gt;
implements Bengio’s original feed-forward neural language models, &lt;a href=&quot;https://www-i6.informatik.rwth-aachen.de/web/Software/rwthlm.php&quot;&gt;RWTHLM&lt;/a&gt;
adds the later RNN and LSTM architectures, while &lt;a href=&quot;https://github.com/senarvi/theanolm&quot;&gt;TheanoLM&lt;/a&gt; lets you customize the networks more easily.
It’s also fairly straightforward to build your own language model in more generic deep learning libraries such as
&lt;a href=&quot;https://www.tensorflow.org/tutorials/recurrent&quot;&gt;Tensorflow&lt;/a&gt;, which is what I did for this blog post. 
There’s a simple example of an LSTM language
model in the &lt;a href=&quot;https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py&quot;&gt;Github Tensorflow repo.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;
I repeated the experiment above with an LSTM-based language model that I trained for ten epochs, using a dimensionality of 256 for the word embeddings
and the LSTM cell. Ideally, we’d like to have more data than just the sentences in George R. R. Martin’s books to train such a model, but 
the results are nevertheless interesting. First of all, the median sentence
perplexities of each episode confirm our earlier findings: with each series, the dialogues in &lt;em&gt;Game of Thrones&lt;/em&gt; get more difficult to predict. Second, the perplexities of our neural language model are generally lower than the ones of the four-gram language model.
This is true in particular for the later series: while the median sentence perplexities of both models average around 38 in the first series,
the last series shows more diverging results: the four-gram language model has a median sentence perplexity of 58 on average, while the LSTM 
model has only 49. This difference showcases the strength of neural language models &amp;mdash; a result of the semantic knowledge they store in their
word embeddings and their ability to look beyond n-gram boundaries.
&lt;/p&gt;

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1Tir_tqVJLQIr9nY868y1dYAwPgM33qSRQbMzR6jFkgU/pubchart?oid=1561749522&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;N-grams will not disappear from NLP any time soon. For simple applications, it’s often more convenient to train an n-gram model
rather than a neural network. Doing so will also save you a lot of time: training an n-gram model on the data above is a matter of seconds, while an 
LSTM can easily need a few hours. Still, when you have the data and the time, neural network models offer some compelling advantages
over their n-gram competitors, such as
their ability to model semantic similarity and long-distance dependencies. 
These days both types of model should be in your toolkit when you’re doing text classification, need to predict the most likely
word in a sentence, or are just having a bit of fun with books and television series.
&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/song-of-ngrams-and-lms/&quot;&gt;Perplexed by Game of Thrones. A Song of N-Grams and Language Models&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on July 05, 2017.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Anything2Vec, or How Word2Vec Conquered NLP]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/anything2vec/" />
  <id>http://www.nlp.town/blog/anything2vec</id>
  <published>2017-04-10T08:00:00-04:00</published>
  <updated>2017-04-10T08:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
Word embeddings are one of the main drivers behind the success of deep learning in Natural Language Processing. Even technical 
people outside of NLP have often heard of word2vec and its uncanny ability to model the semantic relationship between 
a noun and its gender or the names of countries and their capitals. But the success of word2vec extends far beyond the 
word level. Inspired by &lt;a href=&quot;https://github.com/MaxwellRebo/awesome-2vec&quot;&gt;this list of word2vec-like models&lt;/a&gt;, 
I set out to explore embedding methods for a broad variety of linguistic units &amp;mdash; from sentences to tweets and medical concepts.
&lt;/p&gt;

&lt;p&gt;
Although the idea of representing words as continuous vectors has been around for a long time, none of the previous approaches 
have been as successful as word2vec. Popularized in a &lt;a href=&quot;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;series&lt;/a&gt;
 &lt;a href=&quot;http://www.aclweb.org/anthology/N13-1090&quot;&gt;of&lt;/a&gt;
 &lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;papers&lt;/a&gt; by Mikolov and colleagues, word2vec offers two ways of 
training word embeddings: in the continuous bag-of-word (CBOW) model, the context words are used to predict the current word; 
in the skip-gram model, the current word is used to predict its context words. Because semantically similar words occur in 
similar contexts, the resulting embeddings successfully capture semantic properties of their words. Most famously, high-quality 
embeddings can (sometimes) answer analogy questions, such as “man is to king as woman is to _”. Indeed, the semantic (and syntactic) 
information that is captured in pre-trained word2vec embeddings has helped many deep learning models generalize beyond their small 
data sets. It is not surprising, then, that this framework has been so influential, both at the word level and beyond. While 
competitors such as &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot;&gt;GloVe&lt;/a&gt; offer alternative ways of training word vectors, 
other models have tried to extend the word2vec approach to other linguistic units.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/w0faljyphdohs8s/Screenshot%202017-04-08%2020.59.27.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Sense2Vec operates on the level of word “senses” rather than simple words.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
One problem with the original word2vec model is that it maps every word to a single embedding. If a word has several senses, these 
are all encoded in the same vector. To address this problem, Trask and colleagues developed &lt;a href=&quot;https://arxiv.org/abs/1511.06388&quot;&gt;sense2vec&lt;/a&gt;, an adaptation of word2vec 
that uses supervised labels to distinguish between senses. For example, in a corpus that has been labelled with parts of speech, 
&lt;code&gt;bank/noun&lt;/code&gt; and &lt;code&gt;bank/verb&lt;/code&gt; are treated as distinct tokens. Trask et al. show that downstream NLP tasks such as dependency parsing can 
benefit when word2vec operates on this “sense” level rather than the word level. Of course, depending on the type of information you choose
to model, “sense2vec” may or may not be a fitting name for this approach. Senses are more than just combinations of words and and their
parts of speech, but in the absence of large sense-tagged corpora, POS-tagged tokens can be a valid approximation.  
Either way, it is clear that performance on certain NLP tasks can get a boost from working with relevant, finer-grained units than simple words.
&lt;/p&gt;

&lt;p&gt;
Some tasks require even more specific information about a word. Part-of-speech tagging, for instance, can benefit enormously from intra-word information that is encoded in smaller units such as morphemes. The adverbial suffix &lt;em&gt;-ly&lt;/em&gt; in English is a good example. 
For this reason, many neural-network approaches operate at least partially on the character level. 
A good example is &lt;a href=&quot;http://jmlr.csail.mit.edu/proceedings/papers/v32/santos14.pdf&quot;&gt;Dos Santos and Zadrozny’s model for part-of-speech tagging&lt;/a&gt;, which uses a convolutional network to extract character-level features.
However, character embeddings are usually trained in a supervised way. This sets them apart from word2vec embeddings, whose training procedure is fully unsupervised.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/yhelg3l8k43lqum/Screenshot%202017-04-07%2021.47.30.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Skip-thought vectors are trained by having a sentence predict the previous and next sentences in a paragraph.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
Still, not all NLP tasks need such detailed information, and many of them focus on units larger than single words. &lt;a href=&quot;https://arxiv.org/abs/1506.06726&quot;&gt;Kiros et al.&lt;/a&gt; present a direct way of translating the word2vec model to the sentence level: instead of having a word predict its context, they have a sentence predict the sentences around it. Their so-called skip-thought vectors follow the encoder-decoder pattern that is so pervasive in Machine Translation nowadays. First a recursive neural network (RNN) encoder maps the input sentence to a sentence vector, and then another RNN decodes this vector to produce the next (or previous) sentence. Through a series of eight tasks such as paraphrase detection and text classification, this skip-thought framework proves to be a robust way of modelling the semantic content of sentences. 
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/qyjrysvx35ays24/Screenshot%202017-04-07%2022.22.23.png?raw=1&quot; /&gt;
&lt;figcaption width=&quot;500&quot;&gt;Tweet2Vec produces a character-based embedding of tweets.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
Unfortunately, sentences are not always as well-behaved as those modelled by Kiros et al. Social media content in particular is a hard beast to tame. Tweets, for example, do not combine into paragraphs and are riddled by slang, misspellings and special characters. That’s why &lt;a href=&quot;https://arxiv.org/abs/1605.03481&quot;&gt;tweet2vec, a model developed by Dhingra and colleagues&lt;/a&gt; uses a character-based rather than a word-based network. First, a bidirectional Gated Recurrent Unit does a forward and backward pass over all the characters in the tweet. Then the two final states combine to a tweet embedding, and this embedding is projected to a softmax output layer. Dhingra et al. train their network to predict hashtags. The idea is that tweets with the same hashtag should have more or less semantically similar content. Their results show that tweet2vec indeed outperforms a word-based model significantly on hashtag prediction, particularly when the tweets in question contain many rare words. 
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/1iig0ptbzuona1w/Screenshot%202017-04-08%2008.59.11.png?raw=1&quot; /&gt;
&lt;figcaption width=&quot;500&quot;&gt;Doc2Vec combines document and word embeddings in a single model.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
Whether they are word-based or character-based, the sentence-modelling methods above require all input sentences to have the same length (in words or characters). This becomes impractical when you’re dealing with longer paragraphs or documents that vary in length considerably. For this type of data, there is &lt;a href=&quot;https://cs.stanford.edu/~quocle/paragraph_vector.pdf&quot;&gt;doc2vec, an approach by Le and Mikolov&lt;/a&gt; that models variable-length text sequences such as sentences, paragraphs, or full documents. It builds embeddings for both documents and words, and concatenates these embeddings to predict the next word in a sliding-window context. For example, in the sliding window &lt;em&gt;the cat sat on&lt;/em&gt;, the document vector would be concatenated with the word vectors for &lt;em&gt;the cat sat&lt;/em&gt; to predict the next word &lt;em&gt;on&lt;/em&gt;. The document vector is unique to each document; the word vectors are shared between documents. Compared to its competitors, doc2vec has some unique advantages: it takes word order into account, generalizes to longer documents, and can learn from unlabelled data. When the resulting document vectors are used in downstream tasks such as sentiment analysis, they prove very competitive indeed. 
&lt;/p&gt;

&lt;p&gt;
But why stop at paragraphs or documents? The next victim that has fallen prey to the word2vec framework is topic modelling. Traditional topic models such as Latent Dirichlet Allocation do not take advantage of distributed word representations, which could help them model semantic similarity between words. &lt;a href=&quot;https://arxiv.org/abs/1605.02019&quot;&gt;Moody’s lda2vec&lt;/a&gt; aims to cure this by embedding word, topic and document vectors into the same space. His method owes a lot to word2vec, but in lda2vec the vector for a context word is obtained by summing the word vector with the vector of the document in which the word occurs. In order to obtain LDA-like topic distributions, these document vectors are defined as a weighted sum of topic vectors, where the weights are sparse, non-negative and sum to one. Moody’s experiments show that lda2vec produces semantically coherent topics, but unfortunately his paper does not offer an explicit comparison with LDA topics. 
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/36awrn287o6hz15/Screenshot%202017-04-07%2022.04.20.png?raw=1&quot; /&gt;
&lt;figcaption width=&quot;500&quot;&gt;Topic2Vec produces both word and topic embeddings, based on LDA topic assignments.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
&lt;a href=&quot;https://arxiv.org/abs/1506.08422&quot;&gt;Niu and Dai&#39;s topic2vec&lt;/a&gt; is even more similar to word2vec. In the CBOW setting, the context words are used to predict both a word and topic vector; in the skip-gram setting, these two vectors themselves predict the context words. Niu and Dai argue that their topics are more semantically coherent than those produced by LDA, but because they only give a few examples, their argument feels rather anecdotic. Moreover, topic2vec still depends on LDA, as the topic assignments for each word in the corpus are required to train the topic vectors. When it comes to word2vec-like topic embeddings, I’m still to be convinced.
&lt;/p&gt;

&lt;p&gt;
After their conquest of classic NLP tasks, it’s no surprise embedding methods have also found their way to specialized disciplines where large volumes of text abound. One good example is &lt;a href=&quot;https://arxiv.org/abs/1602.05568&quot;&gt;med2vec&lt;/a&gt;, an adaptation of word2vec to the medical domain. Choi and colleagues present a neural network that learns embeddings for medical codes (diagnoses, medications and procedures) and patient visits. Their method differs from word2vec in two crucial respects: it is explicitly modified to produce interpretable dimensions, and it is sensitive to the order of the patient visits. While the quality of the code embeddings in the evaluation is mixed, the embedding dimensions are clearly correlated with specific medical conditions. The visit embeddings in their turn prove quite effective at predicting the severity of the clinical risk and future medical codes. Med2vec is not the only example of embeddings in the medical domain: &lt;a href=&quot;http://www.nature.com/articles/srep26094&quot;&gt;Deep Patient&lt;/a&gt; learns unsupervised representations of patients to predict their medical future on the basis of their electronic health records.
&lt;/p&gt;

&lt;p&gt;
And the list doesn’t end there. In the domain of scientific literature, &lt;a href=&quot;https://researchweb.iiit.ac.in/~soumyajit.ganguly/papers/A2v_1.pdf&quot;&gt;author2vec&lt;/a&gt; learns 
representations for authors by capturing both paper content and co-authorship, while &lt;a href=&quot;https://arxiv.org/pdf/1703.06587.pdf&quot;&gt;citation2vec&lt;/a&gt; embeds papers 
by looking at their citations. And if we leave language for a moment, word2vec-like approaches have been applied to 
&lt;a href=&quot;https://www.scribd.com/document/334578710/Person-2-Vec&quot;&gt;people in a social network&lt;/a&gt;, &lt;a href=&quot;https://github.com/mattdennewitz/playlist-to-vec&quot;&gt;playlists on Spotify&lt;/a&gt;, 
&lt;a href=&quot;https://github.com/warchildmd/game2vec&quot;&gt;video games&lt;/a&gt; and &lt;a href=&quot;https://github.com/airalcorn2/batter-pitcher-2vec&quot;&gt;Major League Baseball Players&lt;/a&gt;. 
Not all of these applications are equally serious, but they all attest to the success of the word2vec paradigm.
&lt;/p&gt;

&lt;p&gt;It’s clear word2vec embeddings are here to stay. Whether the framework is used to embed words or other linguistic units, the resulting vectors have played a huge role in the success of deep learning methods. At the same time, they still have clear limitations. Simple tokens may be relatively easy to embed, but word senses and topics are a different matter. How do we make the embedding dimensions more interpretable? And what can we gain from adding explicit linguistic information beyond word order? In these respects, embeddings are still in their infancy.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/anything2vec/&quot;&gt;Anything2Vec, or How Word2Vec Conquered NLP&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on April 10, 2017.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Understanding Deep Learning Models in NLP]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/understanding-deeplearning-models-nlp/" />
  <id>http://www.nlp.town/blog/understanding-deeplearning-models-nlp</id>
  <published>2017-01-26T07:00:00-05:00</published>
  <updated>2017-01-26T07:00:00-05:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
Deep learning methods have taken Artificial Intelligence by storm. As their dominance grows, one inconvenient truth
 is slowly emerging: we don’t actually understand these complex models very well. Our lack of understanding leads to some uncomfortable questions.
Do we want to travel in self-driving cars whose inner workings no one really comprehends? Can we base important decisions
in business or healthcare on models whose reasoning we don’t grasp? It’s problematic, to say the least.&lt;/p&gt;

&lt;p&gt;In line with the general evolutions in AI, applications of deep learning in Natural Language Processing suffer from this lack of understanding as well. 
It already starts with the word embeddings that often form the first hidden layer of the
network: we typically don’t know what their dimensions stand for. The more hidden layers a model has, the more serious this problem becomes. How do neural networks combine the meanings of
single words? How do they bring together information from different parts of a sentence or text? 
And how do they use all these various sources of information to arrive at their final decision? The complex network
architectures often leave us groping in the dark. Luckily more and more researchers are starting to address exactly these questions,
and a number of recent articles have put forward methods
for improving our understanding of the decisions deep learning models make.&lt;/p&gt;

&lt;p&gt;Of course, neural networks aren’t the only machine learning models that are difficult to comprehend. Explaining a complex
decision by a Support Vector Machine (SVM), for example, is not exactly child’s play. For precisely this reason, 
&lt;a href=&quot;https://arxiv.org/abs/1602.04938&quot;&gt;Riberi, Singh and Guestrin&lt;/a&gt; have developed &lt;a href=&quot;https://github.com/marcotcr/lime&quot;&gt;LIME&lt;/a&gt;,
an explanatory technique that can be applied to a variety of machine learning methods. As it explains how a classifier has come
to a given decision, LIME aims to combine interpretability
with local fidelity. Its explanations are interpretable because they account for the model’s decision with a small number of single words that
have influenced it. They are locally faithful because they correspond well to how the model behaves in the vicinity
of the instance under investigation. This is achieved by basing the explanation on sampled instances that are weighted by their similarity to the target example.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/x1seyfxawrta7id/Screenshot%202017-01-24%2023.27.46.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Ribeiro et al. use a small set of words to explain a classifier’s decision.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Ribeiro and colleagues show that LIME can help expose models whose high accuracy on a test set is due 
to peculiarities of the data rather than their ability to generalize well. This can happen, for example, when a text classifier bases
its decision on irrelevant words (such as &lt;em&gt;posting&lt;/em&gt; or &lt;em&gt;re&lt;/em&gt;) that happen to occur predominantly in one text class,
but that are not actually related to that class. Explanations like the one above can also help people discover noisy features,
and improve the model by removing them. LIME clearly demonstrates how a deeper understanding of the models can lead to a better
application of machine learning.&lt;/p&gt;

&lt;p&gt;Although neural networks are often viewed as black boxes, &lt;a href=&quot;https://arxiv.org/abs/1612.07843&quot;&gt;Leila Arras and colleagues&lt;/a&gt; argue
 that their decisions may actually be easier to interpret than those by some competing approaches. 
To underpin their argument, they apply a technique called layer-wise relevance propagation (LRP). LRP measures how much each individual word in a text
contributes to the decision of a neural network classifier by backward-propagating its output. Step by step, LRP allocates
relevance values for a specific class to all cells in the intermediate
layers. When it reaches the embedding layer, it pools the relevances over all dimensions to obtain word-level relevance scores.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/3kyr078p4s1j8bh/Screenshot%202017-01-22%2012.29.16.png?raw=1&quot; /&gt;
&lt;figcaption width=&quot;500&quot;&gt;Arras et al. show that a CNN classifier focuses on fewer, more meaningful words than an SVM.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
A comparison of these word-level relevance scores between a convolutional neural 
network (CNN) and a support vector machine (SVM) in a document classification task shows that the decisions of the CNN 
are based on fewer, more semantically meaningful
words. 
Thanks to its word embeddings, the neural network deals much better with words that are not present in the
training data, and it is less affected by the peculiarities of word frequency. Moreover, its relevance
scores can be used to compute document-level summary vectors that make much more semantic sense
than those obtained with other weighting schemes, such as tf-idf. In short, neural networks may not only have the quantitative,
but also the qualitative edge over their competitors.
&lt;/p&gt;

&lt;p&gt;Like Arras et al, &lt;a href=&quot;https://arxiv.org/abs/1506.01066&quot;&gt;Li, Chen, Hovy and Jurafsky&lt;/a&gt; try to demystify neural
networks by taking their cue from back-propagation. To find out which words in a sentence make the most significant contribution to the network’s choice
of class label, they look at the first-order derivatives of the loss function with respect to the word
embeddings of the words in the sentence. The higher the absolute values of this derivative, the more a word is responsible
for the decision. With their method, Li and colleagues show that the gating structure of Long Short-Term Memory (LSTM)
networks, and bidirectional LSTMs in particular, allows the network to focus better on the important keywords than standard Recurrent
Neural Networks (RNNs).
 In the sentence “I hate the movie”, 
all three models correctly recognize the negative sentiment mostly because of the word “hate”, but the LSTMs give much less
attention to the other words than the RNN. Other examples, however, indicate that the obtained first-order derivatives are
only a rough approximation of the individual contributions of the words, so they should be interpreted with caution.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/l0esdr7ouqdo0pz/Screenshot%202017-01-24%2022.18.01.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Li et al. visualize the more focused attention of LSTM networks.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Luckily, there are more direct ways of measuring word salience. 
&lt;a href=&quot;https://arxiv.org/abs/1602.08952&quot;&gt;Kádár, Chrupała and Alishahi&lt;/a&gt; explore an erasure technique for RNNs: they 
compare the activation vector produced by the neural network after reading a full sentence with the activation
vector for the sentence with the word removed. The more these two vectors differ, the more
importance the network assigns to the word in question. For example, in the phrase “a pizza sitting
next to a bowl of salad”, an RNN typically assigns a lot of importance to the words “pizza” and “salad”, but 
not to “a” or “of”.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/kr4y1g9euh43y58/Screenshot%202017-01-22%2011.44.41.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Kádár et al. identify important words by erasing them from the input.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To demonstrate the full potential of this method, Kádár et al. apply it to one specific RNN architecture: 
the &lt;a href=&quot;https://arxiv.org/abs/1506.03694&quot;&gt;Imaginet&lt;/a&gt; model they introduced in 2015. 
ImageNet consists of two separate Gated Recurrent Unit (GRU) pathways. 
One of these predicts image vectors given image descriptions; the other is a straightforward language
model. They show that the visual pathway focuses most of its attention on a small
number of elements (mostly nouns), while the textual pathway distributes its attention more equally across
all words and assigns more importance to purely grammatical words (such as prepositions).
The visual pathway focuses more on the first part of the sentence (where the central entity is usually
introduced), whereas the textual pathway is more sensitive to the end of the sentence. Some of these observations
confirm well-known intuitions, while others are more surprising.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/2b79rjo9klun4vy/Screenshot%202017-01-22%2011.45.05.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Kádár et al. show how visual and textual tasks make neural networks focus on different parts of the sentence.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.08220&quot;&gt;Li, Monroe and Jurafsky&lt;/a&gt; also
investigate the behaviour of a neural network by feeding two inputs to the model: first the original input, and then the input without
the word or dimension of interest. Instead of looking at the full activation vector, however, they focus on the 
final (log-)likelihoods that the model assigns to the correct label for the two inputs. The larger the difference
between the two output values, the more important the word or dimension is for that particular decision.&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/ish18zzaqdhinys/Screenshot%202017-01-21%2020.43.00.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Li et al. investigate the influence of individual word embedding dimensions on a variety of NLP tasks.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;With this erasure technique, Li and colleagues discover that some dimensions in &lt;a href=&quot;https://arxiv.org/pdf/1301.3781.pdf&quot;&gt;word2vec&lt;/a&gt; embeddings correlate
with specific NLP tasks, such as part-of-speech tagging and named entity recognition. &lt;a href=&quot;http://www-nlp.stanford.edu/pubs/glove.pdf&quot;&gt;Glove embeddings&lt;/a&gt;, 
by contrast, have one dimension that dominates for almost all tasks they investigate. This dimension
likely contains information about word frequency. When dropout is applied in training, information gets spread out more equally
across the dimensions. The higher layers of the neural model, too, are characterized by more scattered
information, and the final decision of the network is more robust to changes at these levels.
&lt;/p&gt;

&lt;figure class=&quot;padded2&quot;&gt;
&lt;img class=&quot;img-fluid&quot; src=&quot;https://www.dropbox.com/s/0szthtqa87rbaj9/Screenshot%202017-01-21%2020.43.27.png?raw=1&quot; /&gt;
&lt;figcaption&gt;Li et al. show that LSTM networks focus more on sentiment words than RNNs in sentiment analysis.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;
Finally, an application of Li et al.’s technique to sentiment analysis confirms some of their earlier findings.
It again shows that LSTM-based models attach a higher
importance to sentiment words (such as &lt;em&gt;masterpiece&lt;/em&gt; or &lt;em&gt;dreadful&lt;/em&gt;) than standard RNNs. 
Moreover, it illustrates how data scientists can perform error analyses on their models by looking at the words with negative importance
scores for the correct class: the negative scores of these words indicate that their removal improves the decision of the model.&lt;/p&gt;

&lt;p&gt;It’s clear neural networks don’t have to be black boxes. All the studies above show that with the right techniques, we can gain a better
understanding of how deep learning works, and what factors motivate its decisions. This blog post has only scratched
the surface of the many interpretative methods researchers are developing. I for one hope that 
in future applications their efforts will lead to model choices that are not only motivated by higher accuracies, but
also by a deeper grasp of what neural models actually do.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/understanding-deeplearning-models-nlp/&quot;&gt;Understanding Deep Learning Models in NLP&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on January 26, 2017.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[DIY methods for sentiment analysis]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/diy-sentiment-analysis/" />
  <id>http://www.nlp.town/blog/diy-sentiment-analysis</id>
  <published>2016-12-10T07:00:00-05:00</published>
  <updated>2016-12-10T07:00:00-05:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
In &lt;a href=&quot;http://nlp.yvespeirsman.be/blog/off-the-shelf-sentiment-analysis/&quot;&gt;my previous blog post&lt;/a&gt;, I explored the wide variety of off-the-shelf solutions that are available for sentiment analysis. The variation in accuracy, both within and between models, led to the question whether you’re better off building your own model instead of trusting a pre-trained solution. I’ll address that dilemma in this blog post.
&lt;/p&gt;

&lt;p&gt;Training your own sentiment analysis model obviously brings with it its own challenges. First, 
you need a large number of relevant texts that have been labelled with one of the target categories: positive, negative or neutral. If you don’t have such a labelled training set, there may be quick and cheap ways of collecting one &amp;mdash; by scraping it from an online source or crowdsourcing annotations from sites such as Amazon’s &lt;a href=&quot;https://www.mturk.com/mturk/welcome&quot;&gt;Mechanical Turk&lt;/a&gt;. Second, you need to pick a machine learning library to implement your solution. Most generic machine learning libraries usually fit the bill, unless you’re aiming to train a non-standard model. I’m a big fan of &lt;a href=&quot;http://scikit-learn.org/&quot;&gt;Scikit-learn&lt;/a&gt;, which I’ve used for all my experiments here. Third, you need to pick the model that is most appropriate for your task and tune its parameter settings. I’ll evaluate two popular models for sentiment analysis, a Naive Bayes Classifier and a Support Vector Machine. Like in &lt;a href=&quot;http://nlp.yvespeirsman.be/blog/simple-text-classification/&quot;&gt;my blog post about text classification&lt;/a&gt;, I’ll use both unigrams and bigrams as features, and set the maximum number of features to 10,000. I’ll evaluate the models on the data sets from &lt;a href=&quot;http://nlp.yvespeirsman.be/blog/off-the-shelf-sentiment-analysis/&quot;&gt;my previous blog post&lt;/a&gt; for which I have sufficient training data: Amazon reviews about baby products, Amazon reviews about Android apps, and Yelp reviews about hotels, restaurants and other tourist attractions. 
&lt;/p&gt;

&lt;p&gt;
The results on the set of baby product reviews show a mixed picture. The newly trained SVM (the green line in the graph below) beats the best off-the-shelf model, but only by 0.8%. The Naive Bayes Classifier (the orange line) gives comparable performance to the second-best off-the-shelf model, around 6% behind the best one.  

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=1783830508&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;
The results for the Android apps are a bit more outspoken. Our new SVM again outperforms the best off-the-shelf model, and this time it does so by a healthy margin of 2.5%. The Naive Bayes model scores 90.3%, similar to the best two off-the-shelf APIs.

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=1100154712&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;
The results on the Yelp reviews confirm these trends. At 95.6%, the SVM performs 2.7% better than Indico; at 89.3%, the Naive Bayes Classifier is outshone by the top three off-the-shelf solutions.

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=1397300606&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;
These figures look pretty convincing: if you have your own data, the right model will typically outperform even the best off-the-shelf method. However, to put this conclusion into perspective, we need to determine how much labelled data we need to achieve these results. It turns out we need a considerable amount. The blue line in the figures below shows the average accuracy across nine training runs as the training set grows. Obviously, our model gets better as we give it more and more labelled data to learn from. On the Android app reviews, the SVM catches up with the best off-the-shelf method at around 20,000 examples. On the baby product reviews, however, the SVM only starts beating the best off-the-shelf method after it has seen around 80,000 labelled examples. In many applications, that amount of data might be really hard to come by. Whether or not the increased accuracy compared to pre-trained solutions is worth the effort will depend on your specific challenge and needs.

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=223762019&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=1382234680&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;In conclusion, there’s often no easy way to choose between an off-the-shelf solution or a custom-built model for your particular NLP task. Off-the-shelf solutions require no data and little effort. They can give you good quality, but this is far from guaranteed. Moreover, you have no control over the model: you cannot improve it through time, or tweak its parameters to perform better on your data. Custom-built models require considerably more data and effort, but if you have those resources available, they will typically give you a superior model over which you have full control. It’s important to remember that no model is perfect. In fact, in George Box’s words, “all models are wrong, but some are useful”. You just have to figure out which ones are which.
&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/diy-sentiment-analysis/&quot;&gt;DIY methods for sentiment analysis&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on December 10, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Off-the-shelf methods for sentiment analysis]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/off-the-shelf-sentiment-analysis/" />
  <id>http://www.nlp.town/blog/off-the-shelf-sentiment-analysis</id>
  <published>2016-11-23T09:00:00-05:00</published>
  <updated>2016-11-23T09:00:00-05:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
Sentiment analysis is one of the most popular applications of Natural Language Processing. Many companies run software that automatically classifies a text as positive, negative or neutral to monitor how their products are received online. 
Other people use sentiment analysis to conduct political analyses: they &lt;a href=&quot;http://tarsier.monkeylearn.com/&quot;&gt;track the average sentiment in tweets that mention the US presidential candidates&lt;/a&gt;, or show that &lt;a href=&quot;http://varianceexplained.org/r/trump-tweets/&quot;&gt;Donald Trump’s tweets are much more negative than those posted by his staff&lt;/a&gt;. There are even companies that &lt;a href=&quot;http://www.stockfluence.com/&quot;&gt;rely on sentiment analysis to try and predict the stock markets&lt;/a&gt;. But how good is sentiment analysis exactly? And what approach should you take when you’re faced with a sentiment analysis task yourself?&lt;/p&gt;

&lt;p&gt;
In line with the rising popularity of machine learning and artificial intelligence in recent years, the landscape of NLP software has grown increasingly complex. On the one hand, there are cloud APIs that offer off-the-shelf models for many common NLP tasks, such as sentiment analysis, named entity recognition, keyword extraction, etc. Some good examples are &lt;a href=&quot;https://indico.io/&quot;&gt;Indico&lt;/a&gt; and &lt;a href=&quot;https://cloud.google.com/natural-language/&quot;&gt;Google’s Natural Language API&lt;/a&gt;. On the other hand, there are software libraries for machine learning that allow you to build your own custom model, such as &lt;a href=&quot;http://scikit-learn.org/&quot;&gt;Scikit-learn&lt;/a&gt; for Python. In between those two options, you have cloud APIs that allow you to train your own model (&lt;a href=&quot;http://monkeylearn.com/&quot;&gt;Monkeylearn&lt;/a&gt;, for example), and there are software libraries that ship with pre-trained models for some popular tasks (&lt;a href=&quot;http://textblob.readthedocs.io/en/dev/&quot;&gt;Textblob&lt;/a&gt;, or &lt;a href=&quot;http://stanfordnlp.github.io/CoreNLP/&quot;&gt;Stanford CoreNLP&lt;/a&gt;). In this first blog post of two I’ll evaluate a wide range of available off-the-shelf methods for sentiment analysis. I’d like to find out if we can just take an existing API, apply it to a data set, and trust its results. In other words: is it OK to be lazy?
&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;img-fluid padded&quot; src=&quot;https://www.dropbox.com/s/rgg7gesfc7iorci/Screenshot%202016-11-17%2022.09.08.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;
This is the full list of systems I tested:&lt;sup&gt;&lt;a href=&quot;#fn1&quot; id=&quot;ref1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 
&lt;ul class=&quot;nomargin&quot;&gt;
    &lt;li&gt;The &lt;a href=&quot;http://textblob.readthedocs.io/en/dev/advanced_usage.html#sentiment-analyzers&quot;&gt;Naive Bayes&lt;/a&gt; and &lt;a href=&quot;http://textblob.readthedocs.io/en/dev/advanced_usage.html#sentiment-analyzers&quot;&gt;Pattern-based&lt;/a&gt; sentiment analyzers in &lt;a href=&quot;https://textblob.readthedocs.io/en/dev/&quot;&gt;TextBlob&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;http://nlp.stanford.edu/sentiment/&quot;&gt;neural sentiment classifier&lt;/a&gt; in &lt;a href=&quot;http://stanfordnlp.github.io/CoreNLP/&quot;&gt;Stanford CoreNLP&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The sentiment API at &lt;a href=&quot;http://sentiment.vivekn.com&quot;&gt;sentiment.vivekn.com&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;https://cloud.google.com/natural-language/docs/sentiment-tutorial&quot;&gt;sentiment analyzer&lt;/a&gt; of the &lt;a href=&quot;https://cloud.google.com/natural-language/&quot;&gt;Google Cloud Natural Language API&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;https://developer.aylien.com/text-api-demo?text=&amp;amp;language=en&amp;amp;tab=sentiment&amp;amp;mode=document&quot;&gt;document-level sentiment analysis&lt;/a&gt; from &lt;a href=&quot;http://aylien.com/&quot;&gt;Aylien&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;https://indico.io/product&quot;&gt;Sentiment Analysis Text Api&lt;/a&gt; from &lt;a href=&quot;https://indico.io/&quot;&gt;Indico&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The Sentiment Analysis API from &lt;a href=&quot;http://sentigem.com&quot;&gt;Sentigem&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;https://developer.rosette.com/features-and-functions#introduction&quot;&gt;Sentiment endpoint&lt;/a&gt; of &lt;a href=&quot;https://developer.rosette.com/&quot;&gt;Rosette API&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;https://dev.havenondemand.com/apis/analyzesentiment#overview&quot;&gt;Sentiment Analysis API&lt;/a&gt; from &lt;a href=&quot;https://www.havenondemand.com/&quot;&gt;Haven OnDemand&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The &lt;a href=&quot;http://text-processing.com/docs/sentiment.html&quot;&gt;Sentiment Analysis API&lt;/a&gt; from &lt;a href=&quot;http://text-processing.com/&quot;&gt;text-processing.com&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;The pre-trained hotel, restaurant and product sentiment classifiers from &lt;a href=&quot;http://monkeylearn.com/&quot;&gt;MonkeyLearn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;It’s a well-known fact that the performance of an NLP model depends on the similarity between its training data and the data you’re testing it on. To get an idea of this variation in performance, I evaluated the available models on user reviews from four very different domains: movies, baby products, Android apps and tourism. In each of these domains, I compared the performance of the model with a &lt;em&gt;baseline&lt;/em&gt; that always labels a text as positive. Obviously, we’d like to do better than that baseline.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;img-fluid padded&quot; src=&quot;https://www.dropbox.com/s/6kj5kd9vwwjm9qj/Screenshot 2016-11-17 21.51.47.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Three examples will show you what the test data looks like. The following texts should get the labels positive, neutral and negative, respectively: 
&lt;ul class=&quot;nomargin&quot;&gt;
  &lt;li&gt;&lt;em&gt;If you’re looking for something scary, this is the first great horror film of the spooky season.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Avernum is a series of demoware role-playing video games by Jeff Vogel of Spiderweb Software available for Macintosh and Windows-based computers. Several are available for iPad and Android tablet.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;It’s Starbucks only with bad customer service. Baristas with attitude that don’t know their own product. If I&#39;m paying $7.00 for a coffee at least drop the ‘tude.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;
There’s no more classic application of sentiment analysis than movie reviews. In order to make sure none of the models was trained on our data, I collected a balanced set of 500 positive and 500 negative sentences from &lt;a href=&quot;https://www.rottentomatoes.com/&quot;&gt;www.rottentomatoes.com&lt;/a&gt;. As its baseline of 50% is pretty low, it’s no surprise that most of the pre-trained models outperform this threshold. Still, there’s a huge difference in performance between the best model (76.8%) and the worst (45.6%). The best three off-the-shelf systems are Indico (76.8%), IBM AlchemyAPI (73.4%) and Stanford CoreNLP (71.9%). I won’t name and shame the worst ones, since their low performance may say more about the appropriateness of their models for this data set than about their inherent quality.

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=567243840&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;
When we move to baby product reviews, the picture becomes much less positive. This 
time only two off-the-shelf models beat the baseline comfortably: Indico (92.6%) and the pre-trained product model in Monkeylearn (87.5%). Textblob’s Pattern-based classifier comes in third, with 82.5% accuracy. Granted, the baseline of 81.5% positive reviews for this data set is much higher than before, but these disappointing figures put the performance of the off-the-shelf methods in perspective.

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=1616059214&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;
Clearly, the pre-trained models are more at home with the hotel and restaurant reviews from Yelp: most systems beat the baseline on this data set. The best API is again Indico (92.9%), followed closely by Google’s Natural Language API (91.0%), and IBM’s AlchemyAPI (90.4%). Monkeylearn’s pre-trained restaurant and hotel classifiers are in fourth and fifth position, which highlights the importance of choosing the right training data for your application.

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=1047884806&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;Finally, the Android apps do not bring any big surprises. When we look at the data set with only positive and negative examples, three models clearly outperform the baseline: Indico (90.6%), Google (90.5%) and Monkeylearn’s pre-trained product model (87.1%). When we replace 300 random texts by a neutral paragraph from Wikipedia that contains the word Android, and test how well the systems recognize neutral examples, the top three changes: it’s now Indico (80.0%), Haven On Demand (79.1%) and Google (77.8%).
&lt;sup&gt;&lt;a href=&quot;#fn2&quot; id=&quot;ref2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=722567687&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/1c7x9AbxqYblyerp5vC7tMsdzTvDyYbNIJ5h3Oduuduc/pubchart?oid=844180096&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;In short, the results of our experiments show enormous variation &lt;em&gt;between&lt;/em&gt; and &lt;em&gt;within&lt;/em&gt; the available off-the-shelf solutions for sentiment analysis. There is huge variation &lt;em&gt;between&lt;/em&gt; the models: some of them perform really well, while others tend to struggle. The names that most often returned among the best systems are Indico, which came out top for all five data sets, and Google and IBM, the big players in this field. In addition, we see considerable variation &lt;em&gt;within&lt;/em&gt; the models: some of them do very well on one data set, but poorly on another. High quality is possible, but it is far from guaranteed.&lt;/p&gt;

&lt;p&gt;
It’s definitely not OK to be lazy when you need to attack your own sentiment analysis task. To make sure you’re getting the best results possible, it’s crucial you evaluate a variety of models on your specific data.
Given our mixed results, and the effort of comparing the available models, you’d be right to ask yourself whether you’d be better off ignoring the available off-the-shelf solutions, and building your own custom model. That’s a question I’ll answer in my next blog post.
&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;&lt;sup id=&quot;fn1&quot;&gt;1. Some of these APIs offer more than one model for sentiment analysis, for example one in the form of a classifier with discrete labels (positive, negative, neutral) and another in the form of a regression model with continuous sentiment values.&lt;a href=&quot;#ref1&quot; title=&quot;Jump back to footnote 1 in the text.&quot;&gt;↩&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;sup id=&quot;fn2&quot;&gt;2. Note that I tuned the models with continuous variables on 500 different examples, in order to determine the most appropriate cut-off between negative, neutral and positive values.&lt;a href=&quot;#ref2&quot; title=&quot;Jump back to footnote 2 in the text.&quot;&gt;↩&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/off-the-shelf-sentiment-analysis/&quot;&gt;Off-the-shelf methods for sentiment analysis&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on November 23, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Text Classification Made Simple&nbsp;&nbsp;]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/simple-text-classification/" />
  <id>http://www.nlp.town/blog/simple-text-classification</id>
  <published>2016-09-21T08:00:00-04:00</published>
  <updated>2016-09-21T08:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
When you need to tackle an NLP task &amp;mdash; say, text classification or sentiment 
analysis &amp;mdash; the sheer number of available software options can be overwhelming. 
Task-specific packages, generic libraries and cloud APIs 
all claim to offer the best solution to your problem, and it can be hard to decide 
which one to use. In this blog post we’ll take a look at some of the available options for a text classification task, and discover their main advantages
and disadvantages.
&lt;p /&gt;

&lt;p&gt; The NLP task in this blog post is a classic instance of text classification: 
we’ll teach the computer to detect the topic in a news article. The task itself is 
pretty straightforward: the topics (sports, finance, entertainment, etc.) are very 
general, and their number is limited to four or five. We’ll use &lt;a href=&quot;http://goo.gl/JyCnZq&quot;&gt;two freely available data sets&lt;/a&gt; to train and test 
our classifiers: the AG News dataset and the Sogou News dataset. Our focus is
on three available solutions:
FastText, a command-line tool, Scikit-learn, a 
machine learning library for Python, and MonkeyLearn, a cloud service. We’ll keep 
parameter optimization to a minimum and evaluate the off-the-shelf models that the 
various solutions offer.&lt;/p&gt;

&lt;h3&gt;The command-line tool: FastText&lt;/h3&gt;

&lt;p&gt;In July, Facebook’s AI lab released &lt;a href=&quot;https://github.com/facebookresearch/fastText&quot;&gt;FastText&lt;/a&gt;, a command-line tool for building 
word embeddings and classifying text. The open-source software package has been
available on Github since early August. Reactions to the release were mixed. One news source claimed &lt;a href=&quot;https://www.inverse.com/article/19878-facebook-fasttext-natural-language-processing-artificial-intelligence&quot;&gt;“Facebook’s 
FastText AI is the Tesla of Natural Language Processing”&lt;/a&gt;, and “researchers just supercharged 
autonomous artificial intelligence”. &lt;a href=&quot;https://twitter.com/yoavgo/status/751178795323908096&quot;&gt;Other people were less 
enthusiastic&lt;/a&gt;, and pointed out FastText is merely a re-implementation of existing techniques. 
&lt;/p&gt;

&lt;p&gt;Revolutionary or not, FastText is extremely easy to use. When you’ve prepared your training data correctly (one
piece of text per line, with the class of the text prefixed by “__label__”), you 
can train a classifier with a single command:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;
&amp;gt; ./fasttext supervised -input data_train -output model
&lt;/div&gt;

&lt;p&gt;Testing your trained classifier on a set of held-out test data is also a breeze:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;
&amp;gt; ./fasttext test model.bin data_test
&lt;/div&gt;

&lt;p&gt;FastText surely lives up to its name. On my laptop, it takes 3 to 4 seconds to train a 
classifier on the single words of the 120,000 AG training texts. The 450,000 
training samples in the Sogou corpus keep it busy for about 2 minutes. With one optional 
parameter setting, the model takes both single words and bigrams (2-word sequences) as features. 
This doubles the training time to 7 seconds for the AG corpus and 4 minutes for Sogou. That’s impressive.&lt;/p&gt;

&lt;p&gt;The trained classifiers are also pretty accurate. In my experiments, the unigram model with the 
standard settings achieved an accuracy of 91.5% on the AG test set, and 93.2% on the Sogou test 
set. Adding bigrams as features led to an increase in accuracy of 0.2% for AG (91.7%), and 2.3% for 
Sogou (95.5%). These figures are slightly lower than those reported in &lt;a href=&quot;https://arxiv.org/pdf/1607.01759v2.pdf&quot;&gt;the paper that accompanied the release of 
FastText&lt;/a&gt;, possibly due to slight differences in tokenization or parameter settings. The paper 
further demonstrates that this accuracy is state-of-the-art, not only on the AG and Sogou corpora, 
but also on other “sentiment” datasets (although, contrary to what the authors suggest, AG and 
Sogou are topical rather than sentiment data).&lt;/p&gt;

&lt;p&gt;This state-of-the-art performance may be due to FastText’s reliance on neural networks rather 
than more traditional, linear models, where parameters cannot be shared among features. Its 
networks learn low-dimensional representations for all features in a text, and then average these 
to a low-dimensional representation of the full text. The resulting models are therefore able to represent 
similarities between different features. For example, the model might learn that &lt;em&gt;big&lt;/em&gt; and 
&lt;em&gt;large&lt;/em&gt; are near-synonyms, and should be treated in a similar manner. That can be really useful, particularly when you’re classifying short texts.&lt;/p&gt;

&lt;p&gt;It’s clear FastText is a neat little software package that deals with large
volumes of data easily and produces high-quality classifiers. Let’s find out how it compares 
to the competition.&lt;/p&gt;

&lt;h3&gt;The software library: Scikit-learn&lt;/h3&gt;

&lt;p&gt;While FastText only has neural networks to learn a classifier, it can often be worthwhile to 
explore some alternative approaches. That’s where more generic machine learning software libraries 
come in. One great example of such a library is &lt;a href=&quot;http://scikit-learn.org/stable/&quot;&gt;Scikit-learn&lt;/a&gt; for Python. Scikit-learn offers a wealth of 
machine learning approaches and makes it really easy to experiment with various models and parameter 
settings. When you’re doing text classification, its Multinomial Naive Bayes classifier is a simple 
baseline to try out, while its Support Vector Machines can help you achieve state-of-the-art 
accuracy.&lt;/p&gt;

&lt;p&gt;Training and testing a model with Scikit-learn is more involved than with FastText, but not very 
much so. The tutorial &lt;a href=&quot;http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html&quot;&gt;Working 
with Text Data&lt;/a&gt; summarizes the most important steps. Basically, what we need is a pipeline with 
three components: a vectorizer that extracts the features from our texts, a transformer that 
weights these features correctly, and finally, our classifier. In my code below, the 
&lt;code&gt;CountVectorizer&lt;/code&gt; tokenizes our texts and models each text as a vector with the 
frequencies of its tokens. The &lt;code&gt;TfidfTransformer&lt;/code&gt; converts these frequencies to the more 
informative tf-idf weights, before the classifier builds a classification model. Training an SVM 
instead of a Multinomial Naive Bayes model is as simple as replacing &lt;code&gt;MultinomialNB&lt;/code&gt; with 
&lt;code&gt;LinearSVC&lt;/code&gt; on line three; extending the features with bigrams can be done by setting 
the &lt;code&gt;CountVectorizer&lt;/code&gt;&#39;s &lt;code&gt;ngram_range&lt;/code&gt; to &lt;code&gt;(1, 2)&lt;/code&gt;. The 
&lt;code&gt;fit&lt;/code&gt; command on line four trains a model by sending the training data through the 
pipeline; the &lt;code&gt;predict&lt;/code&gt; command uses the trained model to classify the test data. Finally, we measure 
the accuracy by checking how often the predicted label is the same as the test label.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;
text_clf = Pipeline([(&#39;vectorizer&#39;, CountVectorizer()),&lt;br /&gt;
                   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(&#39;transformer&#39;, TfidfTransformer()),&lt;br /&gt;
                   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(&#39;classifier&#39;, MultinomialNB())])&lt;br /&gt;
text_clf.fit(data[&quot;train&quot;][&quot;texts&quot;],data[&quot;train&quot;][&quot;labels&quot;])&lt;br /&gt;
predicted_labels = text_clf.predict(data[&quot;test&quot;][&quot;texts&quot;])&lt;br /&gt;
print(np.mean(predicted_labels == data[&quot;test&quot;][&quot;labels&quot;]))&lt;br /&gt;
&lt;/div&gt;

&lt;p&gt;Compared to FastText, Scikit-learn is painfully slow. For example, training an SVM on the 
unigrams and bigrams of the Sogou corpus takes about 17 minutes on my laptop, compared to 4 minutes 
for FastText. It also requires several times more memory. The models can be made significantly 
smaller and faster by setting a minimum frequency for their features, but when you’re dealing with 
lots of data, speed and memory usage can become a concern.&lt;/p&gt;

&lt;p&gt;In terms of accuracy, however, there is much less difference between the two solutions. In fact, 
I obtained a slightly higher accuracy with Scikit-learn than with FastText: when it is trained on 
unigrams and bigrams, its SVM classifies 92.8% of the AG test examples correctly (FastText: 91.7%, 
92.5% in the paper), and 97.1% of the Sogou examples (FastText: 95.5%, 96.8% in the paper). As 
expected, the Naive Bayes classifier does less well, with an accuracy of 90.8% for AG, and 90.2% 
for Sogou.&lt;/p&gt;

&lt;p&gt;In summary, Scikit-learn is a really versatile library that allows you to quickly experiment 
with many different models and parameter settings. The more advanced of these often obtain a very 
good performance. However, out of the box it’s less suitable for modelling large data sets than 
FastText.&lt;/p&gt;

&lt;h3&gt;The cloud solution: MonkeyLearn&lt;/h3&gt;

&lt;p&gt;A third simple approach to text classification is to use an online API. While 
most available services have pre-trained models for traditional NLP tasks such as named
entity recognition, sentiment analysis, and general text classification, some of them
allow people to train their own models and hopefully achieve a better accuracy on
domain-specific data. One of these is &lt;a href=&quot;http://www.monkeylearn.com/&quot;&gt;MonkeyLearn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the main selling points of MonkeyLearn is its user-friendliness. Three dialogue windows 
help us set up a text classification model. During this process, we tell the system we would like to 
train a classifier, that this classifier should categorize texts by topic, and that our texts are 
news articles in English (or Chinese for Sogou). These settings help Monkeylearn choose the best 
pre-processing steps (filtering, emoticon normalization, etc.) and select the best combination of 
parameters for its models. When this is done, it creates an environment where we can 
manipulate the parameters of our model, train it and explore its performance.&lt;/p&gt;

&lt;img class=&quot;img-fluid padded&quot; src=&quot;https://www.dropbox.com/s/ydxzd31qjc30i8e/Screenshot%202016-08-23%2010.10.38.png?raw=1&quot; /&gt;

&lt;p&gt;Before we can train our model, we need to upload the training data, either as a csv or Excel 
file, or as text data, through a simple API call. Unfortunately, there’s a pretty strict limit on the 
number of training data MonkeyLearn accepts. The free plan allows for 3,000 training examples, the 
Ultra Gorilla for 40,000, and the Enterprise plan is made to measure. These limits may not be an issue when 
there is little tagged data for your problem, but in this age of big data they do feel a bit 
restrictive. Luckily the folks behind MonkeyLearn were friendly enough to give me access to the Ultra 
Gorilla plan for a few weeks. As a result, I chose to work with 40,000 training examples for the AG corpus,
and 20,000 for Sogou.&lt;/p&gt;

&lt;p&gt;While the MonkeyLearn user interface is particularly attractive for newcomers to NLP, more 
experienced users still have the possibility to tweak the main parameters of their classifier. 
These include the type of model (Naive Bayes or SVM), the type of features (unigrams, bigrams, 
trigrams, or a combination of these), the maximum number of features (the default is 10,000), 
the stopwords, etc. There’s also a reference page that explains what these 
parameters mean.&lt;/p&gt;

&lt;img class=&quot;img-fluid padded&quot; src=&quot;https://www.dropbox.com/s/45p69ndljdfea3t/Screenshot%202016-09-18%2010.47.17.png?raw=1&quot; /&gt;

&lt;p&gt;Building a classifier takes a considerable amount of time. For example, MonkeyLearn needs around
20 minutes to train an SVM on the unigrams and bigrams in 20,000 Sogou news texts. Scikit-learn does
that in just 41 seconds on my laptop. However, our patience is rewarded with a
nice overview of our model and its performance: its accuracy, the keywords in our text data, and a 
really useful confusion matrix that helps us analyze what mistakes the classifier tends to make. 
These statistics are based on 4-fold cross-validation on the training data, which explains the longer 
training times at least partially.&lt;/p&gt;

&lt;img class=&quot;img-fluid padded&quot; src=&quot;https://www.dropbox.com/s/1ardbntka29w46l/Screenshot%202016-08-27%2015.42.19.png?raw=1&quot; /&gt;

&lt;p&gt;In order to test the trained model on our test set, we can make use of another of 
MonkeyLearn’s distinguishing features: all trained classifiers are immediately available for 
production, through an API. As with Scikit-learn, we experimented with a Naive Bayes classifier 
and an SVM, both with and without bigrams. On the AG corpus, there is very little difference 
between these settings: all models achieve an accuracy between 87.7% and 88.0%. With the same 
training examples, our Scikit-learn models gave an accuracy of 89.3% (SVM, unigrams only), while
FastText achieved 88.6% (unigrams only). The Sogou corpus shows bigger differences. The 
best-performing MonkeyLearn model is the SVM with unigrams and bigrams, with an accuracy of 
94.2%. This is about the same as a Scikit-learn SVM trained on the same examples (94.8%), and 
considerably better than FastText, which scores 89.1% with unigrams, and 86.8% with ungirams and 
bigrams.&lt;/p&gt;

&lt;p&gt;MonkeyLearn takes away some of the pains of developing a classifier: it assists users in the 
training process, helps them evaluate their models, and immediately exposes their classifiers through 
an API. In terms of accuracy, it’s in the same ballpark as the other approaches we’ve tested
here, but training takes a while and using large data sets can become expensive.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;As machine learning and AI grow in popularity, tackling basic NLP tasks is getting easier and 
easier. The wealth of available options means NLP practitioners are now often spoilt for choice. 
While comparing the accuracy of the models may be a logical first step, it did not reveal a clear 
winner among the three approaches to text classification I tested here. This is true in particular because 
I didn’t do any parameter optimization, and some approaches may have performed better with other 
parameter settings.&lt;/p&gt;

&lt;p&gt;This means other factors will be decisive when you weigh up the different software options. If 
you’re working with big data sets, and speed and low memory usage are crucial, FastText looks 
like a good choice. The figures I obtained with 20,000 examples do suggest it works less well with small data sets, however. If 
you need a flexible framework that allows you to compare a wide range of machine learning models 
quickly, Scikit-learn will help you out. Its best classifiers can give great performance on both 
small and large data sets, but training large models can take some time. If you’re relatively new 
to NLP and looking for a text classifier that’s easy to build and integrate, MonkeyLearn is worth 
checking out. Keep in mind it’s less suitable for big-data problems and you give up some control.&lt;/p&gt;

&lt;p&gt;There’s no doubt there are many more possibilities than the three 
frameworks I’ve explored here. As new options come along frequently,
we’ll be faced with the agony of choice for quite some time to come.	 
Still, a few simple experiments can go a long way to help you take your 
pick.&lt;/p&gt;

&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/simple-text-classification/&quot;&gt;Text Classification Made Simple&nbsp;&nbsp;&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on September 21, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP in the Cloud: Measuring the Quality of NLP APIs]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/nlp-api-ner/" />
  <id>http://www.nlp.town/blog/nlp-api-ner</id>
  <published>2016-06-27T08:00:00-04:00</published>
  <updated>2016-06-27T08:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;
Natural Language Processing seems to have become somewhat of a commodity in recent years. More than a few companies have sprung 
up that offer basic NLP capabilities through a cloud API. If you’d like to know whether a text carries a positive or negative 
message, or what people or companies it mentions, you can just send it to one of these black boxes, and receive the answer 
in less than a second. Superficially, all these NLP APIs look more or less the same. 
&lt;a href=&quot;https://www.textrazor.com/&quot;&gt;Textrazor&lt;/a&gt;,
&lt;a href=&quot;http://www.alchemyapi.com/&quot;&gt;AlchemyAPI&lt;/a&gt;,
&lt;a href=&quot;http://aylien.com/&quot;&gt;Aylien&lt;/a&gt;,
&lt;a href=&quot;https://www.meaningcloud.com/&quot;&gt;MeaningCloud&lt;/a&gt; and
&lt;a href=&quot;https://www.lexalytics.com/&quot;&gt;Lexalytics&lt;/a&gt; all offer similar services (named entity recognition, 
sentiment analysis, keyword extraction, topic identification, etc.), and do so through similar interfaces. However, this surface 
conceals huge differences in quality.   
&lt;/p&gt;

&lt;p&gt;
In this article, I evaluate five NLP APIs on one specific, seemingly simple task: 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Named-entity_recognition&quot;&gt;named entity recognition&lt;/a&gt; (NER) for English. 
In named entity recognition, the goal is to identify so-called named entities, such as locations, people and organizations. 
This is useful for many applications in information extraction, such as search engines or question answering software. 
Most APIs offer more entity types than the three main categories 
(for example, dates or events), but since locations, people and organizations are the most widely accepted and available, 
they are the ones I will focus on. 
&lt;/p&gt;

&lt;p&gt;
To evaluate the output of these APIs, I collected one hundred sentences from a range of news websites, including 
&lt;a href=&quot;http://www.theguardian.com/international&quot;&gt;the Guardian&lt;/a&gt;, 
&lt;a href=&quot;http://www.bbc.co.uk/news&quot;&gt;BBC&lt;/a&gt;, 
&lt;a href=&quot;http://edition.cnn.com/&quot;&gt;CNN&lt;/a&gt;, etc. 
All of these sentences contain at least one entity. They are well-written sentences in grammatical English, so they 
should pose fewer problems than, say, tweets. They cover news facts from different corners of the world and various 
domains (politics, sports, science, etc.), 
so the entities in them are pretty varied. To give an example of the output I expect, take the sentence
&lt;blockquote class=&quot;nomargin&quot;&gt;
A diplomatic fight, usually kept behind closed doors, exploded in public Thursday as U.N. Secretary-General Ban Ki-Moon accused Saudi Arabia and its military allies of placing &quot;undue pressure&quot; on the international organization.
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p class=&quot;noindent&quot;&gt;Good NER software should identify &lt;em&gt;U.N.&lt;/em&gt; as an organization, &lt;em&gt;Ban Ki-Moon&lt;/em&gt; as a person, 
and &lt;em&gt;Saudi Arabia&lt;/em&gt; as a location. 
&lt;/p&gt;

&lt;h3&gt;Technical details&lt;/h3&gt;

&lt;p&gt;
I used this test set of sentences to evaluate the five APIs that are discussed in Robert Dale’s article 
&lt;a href=&quot;http://web.science.mq.edu.au/~rdale/publications/industrywatch/2015-V21-4.pdf&quot;&gt;NLP meets the cloud&lt;/a&gt;: 
&lt;a href=&quot;https://www.textrazor.com/&quot;&gt;Textrazor&lt;/a&gt;, 
&lt;a href=&quot;http://www.alchemyapi.com/&quot;&gt;AlchemyAPI&lt;/a&gt; (which is now part of 
&lt;a href=&quot;http://www.ibm.com/cloud-computing/bluemix/watson/&quot;&gt;IBM Watson&lt;/a&gt;), 
&lt;a href=&quot;http://aylien.com/&quot;&gt;Aylien&lt;/a&gt;, 
&lt;a href=&quot;https://www.meaningcloud.com/&quot;&gt;MeaningCloud&lt;/a&gt; and 
&lt;a href=&quot;https://www.lexalytics.com/&quot;&gt;Lexalytics&lt;/a&gt;. These all offer a free account with a limited number 
of calls that more than suffices for a thorough evaluation of their output.
Interfacing with the API happens through manual REST calls or a custom library in your favorite programming language. 
Some interfaces are more confusing than others (looking at you, Lexalytics), but in general, setting this up is simple enough. 
&lt;/p&gt;

&lt;p&gt;
Per entity type, I use three standard metrics to measure the quality of the API:
&lt;ul class=&quot;nomargin&quot;&gt;
&lt;li&gt;Precision: If an API identifies a word or word sequence as an entity, how likely is this to be correct?&lt;/li&gt;
&lt;li&gt;Recall: What percentage of entities in the text does the API identify correctly?&lt;/li&gt;
&lt;li&gt;F-score: the (harmonic) mean of precision and recall captures the quality of the API for each entity type 
in one single figure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p class=&quot;noindent&quot;&gt;
Here are some additional rules of the game:
&lt;ul class=&quot;nomargin&quot;&gt;
&lt;li&gt;I use the available APIs as off-the-shelf services, without tweaking the results. Some APIs give a confidence score for every entity, which would allow users to filter out entities with a low confidence. This is potentially interesting, but it also takes quite some effort, so I just evaluate the output as-is.&lt;/li&gt;
&lt;li&gt;When an API misclassifies a location as an organization, or vice versa, I don’t penalize it and just follow the 
classification of the API. Place names are often ambiguous (as in &lt;em&gt;Iran said …&lt;/em&gt;), so let’s not be too strict.&lt;/li&gt;
&lt;li&gt;When an API identifies an entity but does not classify it as a person, location or organization, I ignore it.&lt;/li&gt;
&lt;li&gt;Some APIs are able to identify adjectives (such as &lt;em&gt;Chinese&lt;/em&gt;) as locations (&lt;em&gt;China&lt;/em&gt;), others are not. If they do so, I count these entities as correct, but if they don’t, I don’t penalize them. Again, let’s not be too strict.&lt;/li&gt;
&lt;li&gt;Some APIs not only identify entities in a sentence, but also try to determine the real-world entity that it refers to, 
and give a link to the relevant Wikipedia page. Because not all APIs provide it, I ignore this reference, whether
it is correct or not.&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;

&lt;p&gt;
TextRazor’s offering is pretty complex, because it classifies entities as both &lt;a href=&quot;http://wiki.dbpedia.org/&quot;&gt;DBPedia&lt;/a&gt;
 categories and &lt;a href=&quot;http://wiki.freebase.com/wiki/Main_Page&quot;&gt;Freebase&lt;/a&gt; types. 
Not only do these two sources have completely different lists of categories, sometimes the Textrazor classifications do not agree. 
In a sentence such as &lt;em&gt;Iraqi troops begin operation to seize Falluja from Isis&lt;/em&gt;, &lt;em&gt;Isis&lt;/em&gt; is classified 
as a Freebase Organization (correct), but a DBPedia Place (incorrect). In this exercise, I chose to work with the Freebase types.
Additionally, Textrazor tends to give long lists of 
categories for every entity, most of which are really specific and frankly, irrelevant. Are you interested in the fact that 
&lt;em&gt;the United States&lt;/em&gt; is not only a &lt;em&gt;/location/location&lt;/em&gt;, but also a 
&lt;em&gt;/meteorology/cyclone_affected_area&lt;/em&gt;, &lt;em&gt;/fictional_universe/fictional_setting&lt;/em&gt; or 
&lt;em&gt;/travel/travel_destination&lt;/em&gt;, wherever it occurs? I know I’m not.
&lt;/p&gt;

&lt;p&gt;Aylien’s API, too, is rather confusing, but in a different way: the API has two endpoints that can be used for entity extraction 
(Entity and Concept extraction), and their results can be very different. Aylien’s Concept extraction makes use of an external knowledge
base (e.g. DBPedia), whereas its Entity extraction is purely self-contained. Concept extraction makes use of rather specific DBPedia entity types (e.g. Scientist,
OfficeHolder, TennisPlayer, etc. are all types of Person), whereas Entity extraction has just the three main categories (Location, 
Person, Organization). Concept extraction can also return several entity types per entity, whereas Entity extraction sticks to one.&lt;/p&gt;

&lt;p&gt;
The remaining APIs are simpler, and return just one category per identified entity. The table below shows how I mapped them to the three
main entity types.

 &lt;table style=&quot;width:100%&quot;&gt;
  &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;Location&lt;/th&gt;
    &lt;th&gt;Person&lt;/th&gt;
    &lt;th&gt;Organization&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Textrazor&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;/location/location&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;/people/person&lt;/em&gt;, &lt;em&gt;/royalty/noble_title&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;organization/organization&lt;/em&gt;, &lt;em&gt;internet/website&lt;/em&gt;, &lt;em&gt;sports/sports_team&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;AlchemyAPI&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;City&lt;/em&gt;, &lt;em&gt;Country&lt;/em&gt;, &lt;em&gt;StateOrCountry&lt;/em&gt;,
&lt;em&gt;Region&lt;/em&gt;, &lt;em&gt;Facility&lt;/em&gt;, &lt;em&gt;GeographicFeature&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Person&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Company&lt;/em&gt;, &lt;em&gt;Organization&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Aylien&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Location&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Person&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Organization&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Lexalytics&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Place&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Person&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Company&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;MeaningCloud&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Top&amp;gt;Location&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Top&amp;gt;Person&lt;/em&gt;&lt;/td&gt;
    &lt;td&gt;&lt;em&gt;Top&amp;gt;Organization&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt; 

&lt;/p&gt;

&lt;h3&gt;Results&lt;/h3&gt;

&lt;p&gt;
Let’s now take a look at the results, starting with locations. Locations are generally the easiest type of entity to identify. 
Four of the five APIs in this evaluation exercise are best at identifying locations, when compared to organizations or people. 
Still, the differences in quality between the APIs are enormous. Textrazor does a great job, with an F-score of 95.9%.
It finds 99% of all locations in the test sentences (recall), and when it identifies a word or word sequence as a location,
this is correct in 93% of the cases (precision). AlchemyAPI is in second position, with an F-score of 90.8%. Its entities
are also correct in 93% of the cases (precision), but it only finds 89% of all locations.
Likewise, the other three APIs all achieve a precision of 93% and above, but their recall suffers greatly: 
MeaningCloud finds 76% of all locations, Aylien&#39;s Concept extraction 72%, its Entity extraction 68%, and Lexalytics a meagre 53%. 
&lt;/p&gt;

&lt;p&gt;Let me give two examples to
show what this means in practice. In the following sentence, Aylien’s Concept Extraction 
finds &lt;em&gt;Belgium&lt;/em&gt;, but fails to classify it as a person, location or organization.
Its Entity extraction does not identify it at all. Both endpoints miss all four people:
&lt;blockquote class=&quot;nomargin&quot;&gt;
Belgium created the game’s first opening when Lukaku and Fellaini
combined to tee up Nainggolan for a 25-yard drive that Buffon pushed away
&lt;/blockquote&gt;
&lt;p class=&quot;noindent&quot;&gt;And in&lt;/p&gt;
&lt;blockquote class=&quot;nomargin&quot;&gt;
Indian president Pranab Mukherjee arrived in Ivory Coast capital Abidjan on Tuesday for a two-day state visit, the first visit of an Indian president since the establishment of diplomatic relations between the two countries in 1960.
&lt;/blockquote&gt;
&lt;p class=&quot;noindent&quot;&gt;Lexalytics misses both locations (&lt;em&gt;Ivory Coast&lt;/em&gt; and &lt;em&gt;Abidjan&lt;/em&gt;), as well as &lt;em&gt;Pranab Mukherjee&lt;/em&gt; (person).
That’s pretty disappointing.&lt;/p&gt;
&lt;iframe width=&quot;600&quot; height=&quot;371&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/16EIiMRHZHqy6mE9kbMtK4nGSMa-XbqtC7A_nRd2TOJ4/pubchart?oid=2010346884&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;

&lt;/p&gt;
&lt;p&gt;
The results for organizations and people follow a similar pattern. Again, Textrazor and AlchemyAPI are in first and second 
position, respectively. Textrazor is particularly strong at identifying many entities (recall), AlchemyAPI wants to get them right (precision). 
MeaningCloud is a solid third place, while the performance of Aylien depends on the endpoint you use: the Concepts endpoint is better at identifying
well-known people, places and organizations, while for lesser known entities and surnames without a first name, the Entities endpoint mostly does
a better job. Lexalytics achieves a very high precision, but its recall is really low. It’s possible its recall and F-score 
can be improved by setting a lower confidence threshold for the entities.
&lt;iframe width=&quot;599.4557377049182&quot; height=&quot;370.61469858156033&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/16EIiMRHZHqy6mE9kbMtK4nGSMa-XbqtC7A_nRd2TOJ4/pubchart?oid=494995784&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;iframe width=&quot;602&quot; height=&quot;372.10226424361497&quot; seamless=&quot;&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;https://docs.google.com/spreadsheets/d/16EIiMRHZHqy6mE9kbMtK4nGSMa-XbqtC7A_nRd2TOJ4/pubchart?oid=1492503240&amp;amp;format=interactive&quot;&gt;&lt;/iframe&gt;
&lt;/p&gt;

&lt;p&gt;
Some of the more striking results can help illustrate the differences between the APIs. In the sentence 
&lt;blockquote class=&quot;nomargin&quot;&gt;
Jenson Button was an encouraging seventh for McLaren, matching the time of Williams&#39; Valtteri Bottas in sixth.
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p class=&quot;noindent&quot;&gt;both Lexalytics and Aylien’s Entity extraction fail to identify a single entity, although there are two people (&lt;em&gt;Jenson Button&lt;/em&gt; and 
&lt;em&gt;Valtteri Bottas&lt;/em&gt;) and two organizations (&lt;em&gt;McLaren&lt;/em&gt; and &lt;em&gt;Williams&lt;/em&gt;). Textrazor and MeaningCloud find all four 
entities correctly. Aylien’s Concept extraction finds three entities, but fails to identify Williams as an organization. 
AlchemyAPI finds &lt;em&gt;Valtteri Bottas&lt;/em&gt;, but it misses &lt;em&gt;Jenson Button&lt;/em&gt; and misclassifies 
&lt;em&gt;McLaren&lt;/em&gt; and &lt;em&gt;Williams&lt;/em&gt; as people. 
&lt;/p&gt;
&lt;p&gt;
In the sentence
&lt;blockquote class=&quot;nomargin&quot;&gt;
This statement by the vice president of the NFF, Seyi Akinwunmi, is a repeat of homophobic statements made by football officials in the country in the past which were strongly condemned by FIFA.
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;p class=&quot;noindent&quot;&gt;Aylien’s endpoints do not classify a single entity as person, location or organization. MeaningCloud and Lexalytics both find &lt;em&gt;FIFA&lt;/em&gt; (organization), but miss 
&lt;em&gt;NFF&lt;/em&gt; (organization) and &lt;em&gt;Seyi Akinwunmi&lt;/em&gt; (person). Textrazor and AlchemyAPI find all three entities, 
although they both map &lt;em&gt;NFF&lt;/em&gt; to the Norwegian Football Federation, and not the Nigerian one. 
&lt;/p&gt;

&lt;p&gt;
Named entity recognition is a harder task than it might seem at first glance. Even the best APIs occasionally make mistakes. 
In many ways, I’ve tested only the most basic features of named entity recognition here. Things get considerably 
harder when the identified entities have to be linked to their correct real-world counterpart (such as &lt;em&gt;NFF&lt;/em&gt; 
in the sentence above), or when several mentions of the same entity (&lt;em&gt;Donald Trump … Trump … he&lt;/em&gt;) have to be 
mapped to one and the same referent. Similarly, even the best APIs struggle when they have to disambiguate an entity 
and infer, for example, that &lt;em&gt;Southwest&lt;/em&gt; in a context such as &lt;em&gt;drier than normal conditions in the Southwest&lt;/em&gt; 
refers to a location rather than a company. Kudos to AlchemyAPI, which was the only one in this test to get this example right. 
There’s no way this can be done without the basic entity recognition 
that I’ve tested here. 
&lt;/p&gt;

&lt;p&gt;
It is certainly true that the NLP APIs in this article have made basic NLP tasks available
to the masses. However, while the offerings may look similar on the outside, their differences become very clear 
when we measure their performance. In my NER test, Textrazor outperformed all others, but it did so with long 
and often confusing lists of possible entity types. AlchemyAPI’s results were much more straightforward, but 
it found considerably fewer entities. MeaningCloud performed reasonably well on simple examples, but failed to perform
on the more complex ones. Aylien has two endpoints that both struggle at times, and its plans to merge them
should really pay off. Lexalytics has its work cut out. Of course, it is important 
to bear in mind that my evaluation
exercise is rather informal and only looked at 100 sentences typical of one 
linguistic style and domain. It’s always possible that other domains may give different results.
Still, it clearly pays off to compare the various APIs available, so buyer beware!
&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/nlp-api-ner/&quot;&gt;NLP in the Cloud: Measuring the Quality of NLP APIs&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on June 27, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP People: the 2016 NLP job market analysis]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/nlppeople-jobmarket-analysis/" />
  <id>http://www.nlp.town/blog/nlppeople-jobmarket-analysis</id>
  <published>2016-05-29T16:00:00-04:00</published>
  <updated>2016-05-29T16:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt;When the University of Leuven asked me to give a guest lecture in their &lt;a href=&quot;http://www.mai.kuleuven.be/&quot;&gt;Master of Artificial Intelligence&lt;/a&gt; earlier this year, 
one thing I set out to do was to give students an idea of the opportunities in the NLP job market. I contacted Alex and Maxim from &lt;a href=&quot;http://www.nlppeople.com&quot;&gt;NLP People&lt;/a&gt;, 
and they were so kind to give me access to their database of job ads. My analysis of their data brought to light some interesting patterns, 
and was posted on the &lt;a href=&quot;https://nlppeople.com/nlp-job-market-analysis-2016/&quot;&gt;NLPPeople blog&lt;/a&gt; earlier this month. I&#39;m reposting the article here below.&lt;/p&gt;

&lt;p&gt;One year after its launch, &lt;a href=&quot;https://nlppeople.com&quot;&gt;NLPPeople&lt;/a&gt; analysed the hundreds of job ads that had been published on the website during that time. In a &lt;a href=&quot;https://nlppeople.com/nlp-people-job-statistics-one-year-after-the-launch/&quot; target=&quot;_blank&quot;&gt;blog post &lt;/a&gt;we discussed the supremacy of the US in NLP jobs, the imbalance between industry and academia, among other other striking patterns. Almost three years later it’s time to take a fresh look at all the job ads we’ve collected so far, to try and determine how the NLP job market has evolved.&lt;/p&gt;
&lt;p&gt;Some things don’t change, of course: like three years ago, most of the jobs that are advertised on NLPPeople are located in the US or the EU. However, the power balance between these two regions seems to have shifted. In 2012, the US was responsible for over 50% of all job ads. Since then this figure has dropped every year, and in 2015 the US represented only 34% of all jobs. The EU, by contrast, has seen the opposite evolution: starting from 29% in 2012, it represented 50% of all job ads in 2015. Obviously, because the job ads on NLPPeople are not selected from all available jobs at random, the shift that we see here need not reflect the real situation. It may simply mean that NLPPeople has become much more popular in the EU than in the US in recent years. At the very least, however, the strong performance of Europe points to a wealth of available &lt;a href=&quot;https://nlppeople.com/job-tag/nlp/&quot;&gt;NLP&lt;/a&gt; jobs in that part of the world.&lt;/p&gt;
&lt;p&gt;Figure:&lt;br /&gt;
&lt;iframe style=&quot;width: 100%; height: 460px;&quot; src=&quot;https://docs.google.com/spreadsheets/d/1oC5Vlh4ItMBISqWuy5Pu2hENxfOPVjaDhvJZql0efD0/pubchart?oid=1098995564&amp;amp;format=interactive&quot; width=&quot;300&quot; height=&quot;150&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;When we zoom in on individual countries, some additional patterns emerge. Within the EU, the &lt;a href=&quot;https://nlppeople.com/job-tag/united-kingdom/&quot;&gt;United Kingdom&lt;/a&gt; is the clear frontrunner with 800 job ads. That’s more than the next three countries, &lt;a href=&quot;https://nlppeople.com/job-tag/germany/&quot;&gt;Germany&lt;/a&gt; (328), &lt;a href=&quot;https://nlppeople.com/job-tag/ireland/&quot;&gt;Ireland&lt;/a&gt; (228) and &lt;a href=&quot;https://nlppeople.com/job-tag/france/&quot;&gt;France&lt;/a&gt; (208), combined. Relative to country size, Ireland in particular has been very successful at attracting multinationals, creating academic NLP jobs at universities and research centres, and founding startups and spin-offs. Outside of the US and the EU, &lt;a href=&quot;https://nlppeople.com/job-tag/india&quot;&gt;India&lt;/a&gt; is an NLP powerhouse with 238 postings, followed by &lt;a href=&quot;https://nlppeople.com/job-tag/israel/&quot;&gt;Israel&lt;/a&gt; (111), &lt;a href=&quot;https://nlppeople.com/job-tag/canada/&quot;&gt;Canada&lt;/a&gt; (89), &lt;a href=&quot;https://nlppeople.com/job-tag/singapore/&quot;&gt;Singapore&lt;/a&gt; (60) and &lt;a href=&quot;https://nlppeople.com/job-tag/japan/&quot;&gt;Japan&lt;/a&gt; (46).&lt;/p&gt;
&lt;p&gt;&lt;iframe style=&quot;width: 100%;&quot; src=&quot;https://docs.google.com/spreadsheets/d/1oC5Vlh4ItMBISqWuy5Pu2hENxfOPVjaDhvJZql0efD0/pubchart?oid=721334263&amp;amp;format=interactive&quot; width=&quot;300&quot; height=&quot;400&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;Another interesting power balance is that between &lt;a href=&quot;https://nlppeople.com/job-tag/industry/&quot;&gt;industry&lt;/a&gt; and &lt;a href=&quot;https://nlppeople.com/job-tag/academia/&quot;&gt;academia&lt;/a&gt;. NLP has always been a promising domain for practical applications, so it’s no surprise the majority of the jobs advertised on NLPPeople are in industry. However, since 2012 the proportion of jobs in academia has increased steadily. Whereas academia accounted for less than 10% of the jobs in 2012, this share has grown to 32% in 2015. These numbers should again be interpreted with caution, but it’s clear academic interest in NLP is pretty strong indeed.&lt;/p&gt;
&lt;p&gt;&lt;iframe style=&quot;width: 100%;&quot; src=&quot;https://docs.google.com/spreadsheets/d/1oC5Vlh4ItMBISqWuy5Pu2hENxfOPVjaDhvJZql0efD0/pubchart?oid=1130604328&amp;amp;format=interactive&quot; width=&quot;300&quot; height=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;Still, the balance between industry and academia varies considerably between different regions. While academic job postings take up almost 35% of all postings from the EU, in the US fewer than 10% of the jobs are academic. This difference was already obvious in 2012, when we attributed it to the available EU funding for research into language technologies on the one hand, and the strength of the US industrial market on the other. Nevertheless, the proportion of academic jobs has increased in both regions year after year: from 23% to 41% in Europe, and from 4% to 16% in the US.&lt;/p&gt;
&lt;p&gt;&lt;iframe style=&quot;width: 100%;&quot; src=&quot;https://docs.google.com/spreadsheets/d/1oC5Vlh4ItMBISqWuy5Pu2hENxfOPVjaDhvJZql0efD0/pubchart?oid=1074338101&amp;amp;format=interactive&quot; width=&quot;300&quot; height=&quot;375&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;When we focus exclusively on industry, it’s obvious NLP plays an important role in various types of corporations. The companies with the most job ads on NLPPeople can be classified into five broad groups. First, there are the big software firms that use &lt;a href=&quot;https://nlppeople.com/job-tag/natural-language-processing/&quot;&gt;Natural Language Processing&lt;/a&gt; to support their diverse product offering: &lt;a href=&quot;https://nlppeople.com/company/Amazon/&quot;&gt;Amazon&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/Google/&quot;&gt;Google&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/Microsoft&quot;&gt;Microsoft&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/IBM&quot;&gt;IBM&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/eBay/&quot;&gt;eBay&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/Intel/&quot;&gt;Intel&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/Oracle&quot;&gt;Oracle&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/LinkedIn/&quot;&gt;LinkedIn&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/Facebook/&quot;&gt;Facebook&lt;/a&gt; and &lt;a href=&quot;https://nlppeople.com/company/Apple/&quot;&gt;Apple&lt;/a&gt;. Then there are four translation and localisation companies: &lt;a href=&quot;https://nlppeople.com/company/Appen/&quot;&gt;Appen&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/Lionbridge/&quot;&gt;Lionbridge&lt;/a&gt;, &lt;a href=&quot;https://nlppeople.com/company/SDL+plc/&quot;&gt;SDL&lt;/a&gt; and &lt;a href=&quot;https://nlppeople.com/company/VistaTEC/&quot;&gt;VistaTEC&lt;/a&gt;. These typically rely on multilingual language technologies. Next, three companies produce full-fledged NLP software: &lt;a href=&quot;https://nlppeople.com/company/Nuance+Communications/&quot;&gt;Nuance&lt;/a&gt; (speech technology), &lt;a href=&quot;https://nlppeople.com/company/Artificial+Solutions/&quot;&gt;Artificial Solutions&lt;/a&gt; (virtual assistants) and &lt;a href=&quot;https://nlppeople.com/company/Systran/&quot;&gt;Systran&lt;/a&gt; (Machine Translation). Finally, there are two media companies, Bloomberg and Thomson Reuters, and one consultancy company: KPMG. It’s clear most of the companies that post repeatedly on NLPPeople do not focus on NLP exclusively, but use it as a technology to support their products or services. The real NLP firms, which develop a product with NLP at its core, are very active relative to their size, but tend to be rather small.&lt;/p&gt;
&lt;p&gt;&lt;iframe style=&quot;width: 100%;&quot; src=&quot;https://docs.google.com/spreadsheets/d/1oC5Vlh4ItMBISqWuy5Pu2hENxfOPVjaDhvJZql0efD0/pubchart?oid=120905950&amp;amp;format=interactive&quot; width=&quot;300&quot; height=&quot;545&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;A similar ranking of the most active institutions in academia reveals the main NLP hubs in the world. &lt;a href=&quot;https://nlppeople.com/company/University+of+Amsterdam/&quot;&gt;The University of Amsterdam&lt;/a&gt; has published no fewer than 40 job ads, followed by &lt;a href=&quot;https://nlppeople.com/company/Saarland+University/&quot;&gt;Saarland University&lt;/a&gt; (32), the &lt;a href=&quot;https://nlppeople.com/company/The+University+of+Edinburgh/&quot;&gt;University of Edinburgh&lt;/a&gt; (28) and the &lt;a href=&quot;https://nlppeople.com/company/University+of+Sheffield/&quot;&gt;University of Sheffield &lt;/a&gt;(27). NLPPeople appears particularly popular at research institutions in the EU. With the exception of &lt;a href=&quot;https://nlppeople.com/company/Nanyang+Technological+University%2C+Singapore/&quot;&gt;Nanyang Technological University&lt;/a&gt; (Singapore), &lt;a href=&quot;https://nlppeople.com/company/Nanyang+Technological+University%2C+Singapore/&quot;&gt;RMIT University&lt;/a&gt; (Australia), &lt;a href=&quot;https://nlppeople.com/company/National+University+of+Singapore/&quot;&gt;the National University of Singapore&lt;/a&gt; (Singapore) and &lt;a href=&quot;https://nlppeople.com/company/Johns+Hopkins+University/&quot;&gt;Johns Hopkins University&lt;/a&gt; (US), all of the most active institutions are based in Europe. The UK in particular is extremely well-represented.&lt;/p&gt;
&lt;p&gt;&lt;iframe style=&quot;width: 100%;&quot; src=&quot;https://docs.google.com/spreadsheets/d/1oC5Vlh4ItMBISqWuy5Pu2hENxfOPVjaDhvJZql0efD0/pubchart?oid=1845518763&amp;amp;format=interactive&quot; width=&quot;300&quot; height=&quot;580&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;In addition to the regions and institutions, we took a look at the programming languages that are mentioned in the job ads. Three languages stand out: Java (193 mentions), Python (146) and C++ (114). Of these three, the popularity of Java has been fairly stable, while that of Python is growing strongly. In 2015 it even replaced Java at the top of the leaderboard. Matlab (49) and Perl (35) follow at a respectable distance, with the latter in a downward trend. For people interested in landing a job in NLP, this ranking can serve as a guide to discover the most useful programming languages in this field.&lt;/p&gt;
&lt;p&gt;&lt;iframe style=&quot;width: 100%;&quot; src=&quot;https://docs.google.com/spreadsheets/d/1oC5Vlh4ItMBISqWuy5Pu2hENxfOPVjaDhvJZql0efD0/pubchart?oid=1889354055&amp;amp;format=interactive&quot; width=&quot;300&quot; height=&quot;400&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;If we take all these findings together, our fresh analysis of the job ads in the &lt;a href=&quot;https://nlppeople.com&quot;&gt;NLPPeople&lt;/a&gt; database suggests that the NLP job market has become significantly more diverse in recent years. From a US- and industry-dominated domain, NLP has evolved to a field that is attractive to both industry and academia, in various parts of the world. There appears to be no shortage of opportunities for job seekers in NLP, whatever their particular ambitions or interests.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/nlppeople-jobmarket-analysis/&quot;&gt;NLP People: the 2016 NLP job market analysis&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on May 29, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Generating Genre Fiction with Deep Learning]]></title>
  <link rel="alternate" type="text/html" href="http://www.nlp.town/blog/generating-genre-fiction-with-deep-learning/" />
  <id>http://www.nlp.town/blog/generating-genre-fiction-with-deep-learning</id>
  <published>2015-08-09T11:00:00-04:00</published>
  <updated>2015-08-09T11:00:00-04:00</updated>
  <author>
    <name>Yves Peirsman</name>
    <uri>http://www.nlp.town</uri>
    <email></email>
  </author>
  <content type="html">&lt;p class=&quot;first&quot;&gt; These days Deep Learning is everywhere. Neural networks are used for just about every task in Natural Language Processing &amp;mdash; from named entity recognition to sentiment analysis and machine translation. A few months ago, &lt;a href=&quot;http://karpathy.github.io&quot;&gt;Andrej Karpathy&lt;/a&gt;, PhD student at Stanford University, released a small software package for automatically generating texts with a recurrent neural network. I wanted to find out how it performs when it is asked to generate genre fiction, such as fantasy or chick lit.&lt;/p&gt;

&lt;p&gt;Andrej’s model is a character-level language model based on multi-layer LSTMs. Let’s break this mouthful down into its most important components. First, we’re dealing with a language model: this means we need to train it on a text, and through analyzing that text, it will learn to generate language all by itself. Second, it operates on the level of characters. In other words, it analyzes and generates texts character by character. Initially, it doesn’t know anything about words: it just knows there are around 130 characters &amp;mdash; mainly letters, numbers, capitalized letters, spaces and punctuation. During its training, it learns the patterns these characters appear in. It will learn, for example, that when it sees the sequence &lt;em&gt;altho&lt;/em&gt;, the next character is much more likely to be a &lt;em&gt;u&lt;/em&gt; than another &lt;em&gt;o&lt;/em&gt;. In this way, it will learn to build words, and eventually, sentences. Third, the language model is based on multi-layer LSTMs. LSTMs, or Long Short-Term Memories, help it learn longer dependencies than the two or three characters it has just seen. We will see an example of that behaviour below. For the interested reader, there’s more information in &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;Andrej’s blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Earlier blog posts from people in the deep learning community have shown the effectiveness of training this type of model on a single book such as &lt;a href=&quot;http://korbonits.github.io/2015/06/28/Torch-bleeding-edge-DNN-research.html&quot;&gt;Ulysses&lt;/a&gt; or &lt;a href=&quot;http://cpury.github.io/learning-holiness/&quot;&gt;the Bible&lt;/a&gt;, or on works by a single author, such as &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/
&quot;&gt;Shakespeare&lt;/a&gt; or &lt;a href=&quot;https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0&quot;&gt;President Obama’s speeches&lt;/a&gt;. After a few training cycles, the model is able to generate text all by itself that clearly displays the same characteristics as the text it was trained on. After analyzing Obama’s speeches, it produces sentences such as &lt;em&gt;The United States will step up to the cost of a new challenges of the American people that will share the fact that we created the problem&lt;/em&gt;. Training it on Shakespeare results in fragments such as &lt;em&gt;They are away this miseries, produced upon my soul,/ Breaking and strongly should be buried, when I perish/ The earth and thoughts of many states&lt;/em&gt;. Obviously, the results are mostly nonsense, but they bear strong similarities to the texts from that particular author or source.&lt;/p&gt;

&lt;p&gt;I was interested to see what would happen if I trained a similar language model on genre fiction &amp;mdash; 
not just works from one author, but books from several authors in the same genre, such
as chick lit or fantasy. Last year I collected a large corpus of amateur fiction 
from &lt;a href=&quot;https://www.smashwords.com/&quot;&gt;Smashwords&lt;/a&gt;. As all of its books are labelled with one or more genres, this corpus lends itself perfectly to that experiment. I trained 
three different models: one for chick lit, one for historical fantasy and one for mystery
&amp;amp; detective. For those interested in the technical details, I used three 2-layer LSTMs with 512 nodes. For all other settings, I kept the default configuration in Andrej’s code. Each network took as its input some forty-odd books from one particular genre, totalling around
2,250,000 words or 10 MB of data. The fragments below were generated with a so-called &lt;em&gt;temperature&lt;/em&gt; of 0.7 or 0.8, which struck a good balance between confidence and diversity.&lt;/p&gt;

&lt;p&gt;It’s time to dive into the results. Here’s a first fragment of automatically generated chick lit. It takes on the form of a short dialogue:&lt;/p&gt;

&lt;blockquote&gt;
They know the next six months, she said, “I don’t know, when he’s going to have a dinner, as much as he did.”&lt;br /&gt;
“No, did you start the tension?” She chorused at her, and started as he took her arms and looked at the bear. &lt;br /&gt;
“You are in this place. I’m sorry,” she chuckled.&lt;br /&gt;
“I’m going to get out of this bathroom and see all the call of my hands. Sometimes I’m in the kitchen under the kitchen wall.”&lt;br /&gt;
“Okay , he’s happy for you.” &lt;br /&gt;
He continued to smack the back of the ball. &lt;br /&gt;
“You stay around.”&lt;br /&gt;
“I’m sorry to meet you. Don’t worry about what we wanted.” &lt;br /&gt;
“So,” asked Maelyn. “You don&#39;t know what I know where we have a thing.”&lt;br /&gt;
“I think he will be free for me.” &lt;br /&gt;
“What’s the next week?” Jeremy asked. &lt;br /&gt;
“I don’t believe that maintaining how to be the only one who some things have picked out.”&lt;/blockquote&gt;

&lt;p&gt;While this is far from a realistic conversation, the typical elements of chick lit are present. The model has produced meaningful phrases such as &lt;em&gt;he’s happy for you&lt;/em&gt;, &lt;em&gt;he took her arms&lt;/em&gt;, &lt;em&gt;we have a thing&lt;/em&gt; or &lt;em&gt;I think he will be free for me&lt;/em&gt;. Male and female characters are involved, and the topic is never far from the relationship between them. Note also that most sentences are perfectly grammatical, even
though they rarely make sense. My favourite must be &lt;em&gt;Sometimes I’m in the kitchen under the kitchen wall.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here’s another, slightly longer example:&lt;/p&gt;

&lt;blockquote&gt;The form of the weekend. The arrival in the corner of the church came. But they looked for the museum, the only person that there was something about it. God, he was introduced to my first stalker. Serena was all strange that before I would like to make sure that this coffee reaches off the bar like she was born and impressed the talent alone. &lt;br /&gt;
“Come from Heath, but it’s a perfect need of an accourted team.” I had seen a group of majors, but brain. I thought when he put her hand in the floor, which I felt full of water. &lt;br /&gt;
“I’ve got to do anything there here to me. That doesn’t . I’m kind of strange,” I said as he let out a smile. He was sure that after that she ended up slowly parking in her door. For some reason, I hadn’t seen the shop in the California he’d only had time. My heart beat some points. I lived in the form of The Edition’s phone to handle the gaze of my underwear before the demolitional regulation of the adults in his eyes. By the arrival are defensed through the grey composure. I presumed I was the only one who couldn’t help while tears could stand up. I didn’t mean to be better at first on this brother’s activity for the one thing to make it much for me. I went for the sun, and then she found his hand it was a very salary she expressed to her hotel. &lt;br /&gt;
“So our whole left will get out of the day, we’re just in a little consultation.” &lt;br /&gt;
“I know. It would be a bit positive. It’s just a great time . I want to go to the car.” My face notices and should be patient in between. Even he played to bed, and my chest means much to make sure that by a milk. When I was doing the telling love when we stopped to realise that I wasn’t the only way to make all that. I was the last time you were glad you invited them for the excess ones. What about his woman, I smile. I can’t complete all this. They’re in its number, so you’ve come to just change my things. I’m a sister.
&lt;/blockquote&gt;

&lt;p&gt;Again, romance is never far off (see &lt;em&gt;I’m kind of strange, I said as he let out a smile&lt;/em&gt; or &lt;em&gt;then she found his hand&lt;/em&gt;). There is talk of rivalry (&lt;em&gt;What about his woman, I smile&lt;/em&gt;) and stalking (&lt;em&gt;he was introduced to my first stalker&lt;/em&gt;). And while there are many phrases I don’t know what to make of (&lt;em&gt;the gaze of my underwear&lt;/em&gt; or &lt;em&gt;I was doing the telling love&lt;/em&gt;), but I’m sure people with a more lively imagination won’t have any problem dreaming up a fine explanation.&lt;/p&gt;

&lt;p&gt;Next, I trained the model on forty-four books of historical fantasy. This made it produces passages such as this one:&lt;/p&gt;

&lt;blockquote&gt;
He heard a soldier on the road and the children came here out of the concern. I stood slowly and reached for my own face. I was standing in the still steps to the mountains the other was my last boy. Hywel began to cover in a cry. &lt;br /&gt;
“You are getting the flower of the dragon for my days for not to act as the fallen streets -- they somehow come out the shadow of the air, as if the gods were begging,” I replied. &lt;br /&gt;
“I can find the road.” &lt;br /&gt;
An angel blow, truth and reveals of the line of a messenger that stretched into the streets of Brechalon. The soldiers ate quietly to his father’s men, completely still and the old fool said the power of respecting a chance, and they would stay in the face of the pilgrim, and I was interrupted by the other man. She had not lived down the side of the choir, a stone -- and it would be more than anything that had been still to see and they brought their eyes upon him, but it had been a lot of the first thing to trouble their thanks. The celebration who stood in the hall. He had the whole of the men, while Azkun laughed as he attempted to simply felt a strange knight. He felt his mouth in the north, and his voice was the demon he had seen again. In the darkness, he had not found a few and a consequence of water that enjoyed the first thing to have, but the dropped completely he was, bound the table by a small, measured form. The girl fell silent to the cross from the field. At the same time, the sun touched the door, the crossbowman approaching the continue in the marble stairs. It was the same one, she pursued the dragon on the ground. He shrugged and swing the knife in careful glance. “We want to get some nights ago.” I pulled her with it in a storm and helped her to him. 
&lt;/blockquote&gt;

&lt;p&gt;This fragment couldn’t be further removed from the previous ones. Instead of stalkers and sisters, it features soldiers, dragons, knights and demons. The scene conjures up an image of beleaguered villages, violent sword fights and a general atmosphere of fear and dread. I would argue it captures the genre of historical fantasy pretty well.&lt;/p&gt;

&lt;p&gt;The third and final genre I played with was mystery and detective. Here’s what the model came up with:&lt;/p&gt;

&lt;blockquote&gt;
“Dead, maybe something supposed to be.”&lt;br /&gt;
“Interesting, Di. But if you will take a clear paper,” Steve said , then nodded. I stepped and slammed onto the building. &lt;br /&gt;
“No, he’s together and don’t know me on the stage.” &lt;br /&gt;
“Then it’s exactly and you’re well happy with me.”&lt;br /&gt;
“He’s never too at the demise of that amount of conversation with the phone down. It’s my background, about five years ago. And it can’t change the voice.” &lt;br /&gt;
She grabbed his face to his feet. &lt;br /&gt;
“That’s incident, I think. I was caught with our accident.” &lt;br /&gt;
“The older man who would actually escape but he will need to have a big plot about his mother.” &lt;br /&gt;
I dropped a drink and saw that a hull of in this case smelled of laptop. &lt;br /&gt;
“Okay , huh?” I thought then who, he didn’t know what you’re under, so I put it careful to drink in bitching mind to make the first thing. Even as first it should be Lord Rushton and Collin who had reached his file, Mrs. Johnson 15, suddenly watching her that I had been. It was the fact that he’d met him near the Civil Search, and the time in making documents, it was a story with what that was a murder. And if I had discomfort with him for leaving anything that was worse, while you’ve got it in his house in the air and Evangeline offered to be a man’s heart to pay from the people. &lt;br /&gt;
“He pressed his girl and jerked his arms around her breasts. &lt;br /&gt;
“If he didn’t say anything but is my life on his Anthony, now,” he said.&lt;br /&gt; 
“I’ve never wanted to be more sort of time to give the address,” she said. “And let’s go.” She groaned and feared she was such a comment of her supplier as he washed on the table. &lt;br /&gt;
“What are you talking about ?” &lt;br /&gt;
“Everyone in the wrong minute was so sailing the man in his flashlight.” Maggie shook her head. &lt;br /&gt;
&lt;/blockquote&gt;

&lt;p&gt;Like many detective stories, this fragment starts with a body. It later mentions an &lt;em&gt;accident&lt;/em&gt; and a &lt;em&gt;murder&lt;/em&gt;, often the competing explanations offered for someone’s &lt;em&gt;demise&lt;/em&gt;. Its general topic is less focused than in the historical fantasy fragment above, but these lexical clues nevertheless link it to mystery and detective.&lt;/p&gt;

&lt;p&gt;Here’s a final example, again by the mystery and detective model:&lt;/p&gt;

&lt;blockquote&gt;
It felt as if she came in or happen, but I think Will, if she was really under the lab, he was about to score off the company and something of the day back into the front of the house with both like a captain. There was no more secret back at the shop. I had to know where the little committee was murdered.”&lt;br /&gt; 
“I’m the best side of the second difference,” I said.&lt;br /&gt; 
“Nothing. The construction way. I’m sorry I don’t have to talk to you.”&lt;br /&gt;  
“The man? I’ll be the one who was the only one who has a problem.” The two girls would be back and forth to Steve’s perfect statement and looked at him with a hard hand beneath the car on the back panel of the gloom.&lt;br /&gt;  
“Suppose the speaker was the Sergeant.”&lt;br /&gt; 
“You all really like to waste a glass of the contrary?”&lt;br /&gt; 
“No, I was, but I know you’re all the day.” He smiled.&lt;br /&gt;  
“I wouldn’t want a stranger off and say it would be said, but it’s fine. There was a brief story.”&lt;br /&gt; 
“Oh , I know you don’t really do more,” she said, “okay, I can. I’ll need to start a fifty feet on top of these time.”&lt;br /&gt;  
“But what do you mean?”&lt;br /&gt;  
“Not so,” Jason said, “it says the men were coming in here.”&lt;br /&gt;  
“Thank you for respecting the way,” Chet said. “I have a professional less than a life of the news and the painting of the jail when it was going to have to tell me.”&lt;br /&gt;  
“I think I can not say that. Wouldn’t you be able to help you?” Ronnie took a step back.&lt;br /&gt; 
“That’s a low, lid, but I will never begin to the right seat.” She took his shoulder to her feet and shake his head around.&lt;br /&gt;  
“I think I’m all right.” I was positive to explain her back.&lt;br /&gt;  
“Well, if you can have some more attempt to leave the window’s walls and the others killed Stephen.” The police taste, a warm slipper branches. As if I’d been recognising the choice with the employees? It didn’t matter. She thought for a second to come and then ask her.&lt;br /&gt; 
&lt;/blockquote&gt;

&lt;p&gt;Again, the influence of the detective genre can be recognized in a number of lexical clues, such as &lt;em&gt;killed&lt;/em&gt;, &lt;em&gt;police&lt;/em&gt;, &lt;em&gt;jail&lt;/em&gt; and &lt;em&gt;statement&lt;/em&gt;. Some sentences could even come directly from a detective novel, such as &lt;em&gt;I had to know where the secret committee was murdered&lt;/em&gt; or &lt;em&gt;I’m sorry I don’t have to talk to you&lt;/em&gt;. In short, the model has reliably picked up some of the most typical characteristics of all three genres I have explored. 

&lt;p&gt;Although it’s good to see the neural networks produce genre-specific words, what I find even more impressive is their ability to imitate language at a lower level. Remember that these are character-level language models that produce one character (letter, symbol, space, etc.) at a time, given the preceding context. Now consider for a moment some of the things these LSTMs have learnt:

&lt;ul&gt;
    &lt;li&gt;A lexicon: with very few exceptions, almost all words in the samples are existing English words.&lt;/li&gt;
    &lt;li&gt;Syntax: nonsensical as they may be, many sentences are grammatically correct. They generally contain a subject and a main verb, transitive verbs mostly have a direct object, singular nouns are preceded by articles, and so on.&lt;/li&gt;
    &lt;li&gt;Spelling: All sentences and names, and only sentences and names, begin with capital letters.&lt;/li&gt;
    &lt;li&gt;Punctuation: All sentences end in a full stop, and most opening quotation marks are followed at some later point by closing quotation marks. This is probably due to the long-term memory capabilities of LSTMs, which are able to remember that there are opening quotation marks in the preceding context, and only forget about them when these have been closed again.&lt;/li&gt;
&lt;/ul&gt;

That’s pretty impressive.&lt;/p&gt;

&lt;p&gt;Given the nonsensical nature of all fragments above, deep learning models aren’t going to replace novelists any time soon. Still, they are already able to learn some of the most typical characteristics of genre-specific language. This behaviour could help us classify stories by their genre or help recognise the author of a book. Moreover, the appeal of these models is much wider than language or literature &amp;mdash; the same neural networks can be &lt;a href=&quot;https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music&quot;&gt;applied to music&lt;/a&gt;, for instance. And with deep neural networks becoming better almost by the day, who knows what will become possible in the future.&lt;/p&gt;
 
&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://www.nlp.town/blog/generating-genre-fiction-with-deep-learning/&quot;&gt;Generating Genre Fiction with Deep Learning&lt;/a&gt; was originally published by Yves Peirsman at &lt;a href=&quot;http://www.nlp.town&quot;&gt;NLP Town&lt;/a&gt; on August 09, 2015.&lt;/p&gt;</content>
</entry>

</feed>
